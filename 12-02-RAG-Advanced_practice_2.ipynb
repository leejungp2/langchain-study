{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12-02 Practice 2: PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 환경설정\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API KEY 를 설정합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API 키를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API 키 정보 로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "CH12-RAG-practice\n"
     ]
    }
   ],
   "source": [
    "# LangSmith 추적을 설정합니다. https://smith.langchain.com\n",
    "!pip install -qU langchain-teddynote\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"CH12-RAG-practice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 추가된 코드\n",
    "#%pip install --upgrade \"pydantic>=2.7.4\" langchain langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 \n",
    "from langchain import hub\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma, FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<코드 비교>\n",
    "- bs4.SoupStrainer(\"main\", attrs={\"id\": [\"main-content\"]}) → <main> 태그 중에서 id=\"main-content\"를 가진 특정 요소의 텍스트만 가져옴.\n",
    "- 페이지의 특정 영역만 크롤링할 때 유용.\n",
    "- 예를 들어 네이버 뉴스나 블로그의 본문만 가져올 때 적합."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF 로드\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서의 수: 43\n",
      "\n",
      "[페이지내용]\n",
      "language models can generate chains of thought if demonstrations of chain-of-thought reasoning are\n",
      "provided in the exemplars for few-shot prompting.\n",
      "Figure 1 shows an example of a model producing a chain of thought to solve a math word problem\n",
      "that it would have otherwise gotten incorrect. The chain of thought in this case resembles a solution\n",
      "and can interpreted as one, but we still opt to call it a chain of thought to better capture the idea that it\n",
      "mimics a step-by-step thought process for ar\n",
      "\n",
      "[metadata]\n",
      "{'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-01-12T01:06:30+00:00', 'author': '', 'keywords': '', 'moddate': '2023-01-12T01:06:30+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data2/test_paper_2.pdf', 'total_pages': 43, 'page': 1, 'page_label': '2'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# PDF 파일 로드. 파일의 경로 입력\n",
    "loader = PyPDFLoader(\"data2/test_paper_2.pdf\")\n",
    "\n",
    "# 페이지 별 문서 로드\n",
    "docs = loader.load()\n",
    "text = docs[2].page_content[:500]\n",
    "print(f\"문서의 수: {len(docs)}\")\n",
    "\n",
    "# 10번째 페이지의 내용 출력\n",
    "print(f\"\\n[페이지내용]\\n{docs[2].page_content[:500]}\")\n",
    "print(f\"\\n[metadata]\\n{docs[1].metadata}\\n\") #전체 페이지 수, 파일 경로 등의 정보 포함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<코드 설명>\n",
    "- docs = loader.load()는 PDF 파일을 페이지 단위로 나눈 Document 객체 리스트를 반환하는 상태\n",
    "- text는 문자열(str)만 추출하므로 사용하려면 Document 객체 리스트로 반환하는 작업이 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `docs[10]`\n",
    "    - docs 리스트는 여러 개의 문서를 저장하는 리스트.\n",
    "    - docs[10]은 0부터 시작하는 인덱싱 기준으로 11번째 문서를 가져옴.\n",
    "- `.page_content[:500]`\n",
    "    - docs[10].page_content → 문서의 내용 (텍스트)\n",
    "    - [:500] → 처음 500자까지만 출력하여 긴 텍스트를 잘라서 보여줌.\n",
    "- `.metadata`\n",
    "    - docs[10].metadata → 문서의 메타데이터(정보) 출력.\n",
    "    - 보통 문서의 제목, URL, 작성일, 출처 등 추가적인 정보가 포함됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문서 분할(Split Documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) CharacterTextSplitter\n",
    "\n",
    "이것은 가장 간단한 방법입니다. 이 방법은 `문자를 기준으로 분할`합니다(기본값은 \"\\n\\n\") 그리고 청크의 길이를 문자의 수로 측정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=10,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['language models can generate chains of thought if demonstrations of chain-of-thought reasoning are\\nprovided in the exemplars for few-shot prompting.\\nFigure 1 shows an example of a model producing a chain of thought to solve a math word problem\\nthat it would have otherwise gotten incorrect. The chain of thought in this case resembles a solution\\nand can interpreted as one, but we still opt to call it a chain of thought to better capture the idea that it\\nmimics a step-by-step thought process for ar']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=100, chunk_overlap=10, separator=\"\\n\\n\"\n",
    ")\n",
    "text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 두 개의 줄바꿈(\\n\\n)을 기준으로 문서를 나눔\n",
    "- 즉, 문단 단위로 텍스트를 분할하려고 할 때 유용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 101, which is longer than the specified 100\n",
      "Created a chunk of size 109, which is longer than the specified 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['language models can generate chains of thought if demonstrations of chain-of-thought reasoning are',\n",
       " 'provided in the exemplars for few-shot prompting.',\n",
       " 'Figure 1 shows an example of a model producing a chain of thought to solve a math word problem',\n",
       " 'that it would have otherwise gotten incorrect. The chain of thought in this case resembles a solution',\n",
       " 'and can interpreted as one, but we still opt to call it a chain of thought to better capture the idea that it',\n",
       " 'mimics a step-by-step thought process for ar']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=10, separator=\"\\n\")\n",
    "text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['language models can generate chains of thought if demonstrations of chain-of-thought reasoning',\n",
       " 'reasoning are\\nprovided in the exemplars for few-shot prompting.\\nFigure 1 shows an example of a model',\n",
       " 'of a model producing a chain of thought to solve a math word problem\\nthat it would have otherwise',\n",
       " 'otherwise gotten incorrect. The chain of thought in this case resembles a solution\\nand can',\n",
       " 'can interpreted as one, but we still opt to call it a chain of thought to better capture the idea',\n",
       " 'the idea that it\\nmimics a step-by-step thought process for ar']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=10, separator=\" \")\n",
    "text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 공백(스페이스 \" \")을 기준으로 텍스트를 나눔\n",
    "- 문장이 아니라 단어 단위로 청크를 나누려는 목적일 때 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100, separator=\" \")\n",
    "# text 파일을 청크로 나누어줍니다.\n",
    "text_splitter.split_text(text)\n",
    "\n",
    "# document를 청크로 나누어줍니다.\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "len(split_docs)\n",
    "#print(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-01-12T01:06:30+00:00', 'author': '', 'keywords': '', 'moddate': '2023-01-12T01:06:30+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data2/test_paper_2.pdf', 'total_pages': 43, 'page': 0, 'page_label': '1'}, page_content='Chain-of-Thought Prompting Elicits Reasoning\\nin Large Language Models\\nJason Wei Xuezhi Wang Dale Schuurmans Maarten Bosma\\nBrian Ichter Fei Xia Ed H. Chi Quoc V . Le Denny Zhou\\nGoogle Research, Brain Team\\n{jasonwei,dennyzhou}@google.com\\nAbstract\\nWe explore how generating a chain of thought —a series of intermediate reasoning\\nsteps—signiﬁcantly improves the ability of large language models to perform\\ncomplex reasoning. In particular, we show how such reasoning abilities emerge\\nnaturally in sufﬁciently large language models via a simple method called chain-of-\\nthought prompting , where a few chain of thought demonstrations are provided as\\nexemplars in prompting.\\nExperiments on three large language models show that chain-of-thought prompting\\nimproves performance on a range of arithmetic, commonsense, and symbolic\\nreasoning tasks. The empirical gains can be striking. For instance, prompting a\\nPaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art\\naccuracy on the')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) RecursiveTextSplitter\n",
    "- 이 텍스트 분할기는 `일반 텍스트에 권장`되는 텍스트 분할기입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain 패키지에서 RecursiveCharacterTextSplitter 클래스를 가져옵니다.\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "recursive_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # 정말 작은 청크 크기를 설정합니다.\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=10,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language models can generate chains of thought if demonstrations of chain-of-thought reasoning\n",
      "reasoning are\n",
      "provided in the exemplars for few-shot prompting.\n",
      "Figure 1 shows an example of a model\n",
      "of a model producing a chain of thought to solve a math word problem\n",
      "that it would have otherwise\n",
      "otherwise gotten incorrect. The chain of thought in this case resembles a solution\n",
      "and can\n",
      "can interpreted as one, but we still opt to call it a chain of thought to better capture the idea\n",
      "the idea that it\n",
      "mimics a step-by-step thought process for ar\n",
      "============================================================\n",
      "language models can generate chains of thought if demonstrations of chain-of-thought reasoning are\n",
      "provided in the exemplars for few-shot prompting.\n",
      "Figure 1 shows an example of a model producing a chain of thought to solve a math word problem\n",
      "that it would have otherwise gotten incorrect. The chain of thought in this case resembles a\n",
      "a solution\n",
      "and can interpreted as one, but we still opt to call it a chain of thought to better capture the\n",
      "the idea that it\n",
      "mimics a step-by-step thought process for ar\n"
     ]
    }
   ],
   "source": [
    "character_text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=100, chunk_overlap=10, separator=\" \"\n",
    ")\n",
    "for sent in character_text_splitter.split_text(text):\n",
    "    print(sent)\n",
    "print(\"===\" * 20)\n",
    "recursive_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100, chunk_overlap=10\n",
    ")\n",
    "for sent in recursive_text_splitter.split_text(text):\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<비교>\n",
    "\n",
    "`CharacterTextSplitter` → 단순히 문자 개수 기준으로 나눔.\n",
    "- 속도가 빠름\n",
    "- 문장 경계를 보장하지 않음\n",
    "- 공백(separator)을 기준으로 나눌 수도 있음\n",
    "\n",
    "`RecursiveCharacterTextSplitter` → 문장을 최대한 자연스럽게 유지하면서 나눔.\n",
    "- 문단 → 문장 → 단어 순서로 작은 단위로 분할\n",
    "- 문장 경계를 유지하려고 함\n",
    "- 문서 요약, 검색 시스템 등에서 더 적합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\n', '\\n', ' ', '']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recursive_text_splitter 에 기본 지정된 separators 를 확인합니다.\n",
    "recursive_text_splitter._separators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Semantic Similarity\n",
    "- `의미적 유사성`을 기준으로 텍스트를 분할합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "출처: [Greg Kamradt’s Notebook](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/5_Levels_Of_Text_Splitting.ipynb)\n",
    "\n",
    "높은 수준(high level)에서 문장으로 분할한 다음 3개 문장으로 그룹화한 다음 임베딩 공간에서 유사한 문장을 병합하는 방식입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 최신 버전으로 업데이트합니다.\n",
    "%pip install -U langchain langchain_experimental -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# SemanticChunker 를 생성합니다.\n",
    "semantic_text_splitter = SemanticChunker(OpenAIEmbeddings(), add_start_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language models can generate chains of thought if demonstrations of chain-of-thought reasoning are\n",
      "provided in the exemplars for few-shot prompting. Figure 1 shows an example of a model producing a chain of thought to solve a math word problem\n",
      "that it would have otherwise gotten incorrect.\n",
      "============================================================\n",
      "The chain of thought in this case resembles a solution\n",
      "and can interpreted as one, but we still opt to call it a chain of thought to better capture the idea that it\n",
      "mimics a step-by-step thought process for ar\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 논문의 일부 내용을 불러옵니다\n",
    "for sent in semantic_text_splitter.split_text(text):\n",
    "    print(sent)\n",
    "    print(\"===\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 임베딩\n",
    "= 임베딩(Embedding)**이란 텍스트(단어, 문장, 문서)를 숫자로 변환하는 기법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wd/rcmpfdjj6pb46wxhh3g97cvr0000gn/T/ipykernel_21743/1940278171.py:10: LangChainDeprecationWarning: Default values for HuggingFaceBgeEmbeddings.model_name were deprecated in LangChain 0.2.5 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceBgeEmbeddings constructor instead.\n",
      "  documents=splits, embedding=HuggingFaceBgeEmbeddings()\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=100, chunk_overlap=10, separator=\"\\n\\n\"\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents=splits, embedding=HuggingFaceBgeEmbeddings()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<설명>\n",
    "- 텍스트 데이터를 벡터로 변환하여 저장하고 검색할 수 있도록 벡터스토어(Vectorstore)를 생성하는 과정\n",
    "- Hugging Face의 BGE 임베딩 모델을 활용하여 FAISS(Vector Database)에 저장하는 방식이 사용됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#!pip uninstall fastembed -y\n",
    "%pip install fastembed -U -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=FastEmbedEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이 코드 원래 에러 났었는데 정상 작동\n",
    "- !pip -> %pip install 관련?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4단계: 벡터스토어 생성(Create Vectorstore)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "= 벡터 스토어(Vector Store)는 텍스트 데이터를 벡터(숫자)로 변환하여 저장하고, 유사한 벡터를 빠르게 검색할 수 있도록 하는 데이터베이스\n",
    "\n",
    "즉, 텍스트를 의미적으로 검색할 수 있도록 변환하는 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# FAISS DB 적용\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Chroma DB 적용\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retriever 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "유사도 기반 검색\n",
    "\n",
    "- 기본값은 코사인 유사도인 `similarity` 가 적용되어 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'author': '', 'creationdate': '2023-01-12T01:06:30+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2023-01-12T01:06:30+00:00', 'page': 2, 'page_label': '3', 'producer': 'pdfTeX-1.40.21', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': 'data2/test_paper_2.pdf', 'subject': '', 'title': '', 'total_pages': 43, 'trapped': '/False'}, page_content='language models can generate chains of thought if demonstrations of chain-of-thought reasoning are\\nprovided in the exemplars for few-shot prompting.\\nFigure 1 shows an example of a model producing a chain of thought to solve a math word problem\\nthat it would have otherwise gotten incorrect. The chain of thought in this case resembles a solution\\nand can interpreted as one, but we still opt to call it a chain of thought to better capture the idea that it\\nmimics a step-by-step thought process for arriving at the answer (and also, solutions/explanations\\ntypically come after the ﬁnal answer (Narang et al., 2020; Wiegreffe et al., 2022; Lampinen et al.,\\n2022, inter alia )).\\nChain-of-thought prompting has several attractive properties as an approach for facilitating reasoning\\nin language models.\\n1.First, chain of thought, in principle, allows models to decompose multi-step problems into\\nintermediate steps, which means that additional computation can be allocated to problems\\nthat require more reasoning steps.\\n2.Second, a chain of thought provides an interpretable window into the behavior of the model,\\nsuggesting how it might have arrived at a particular answer and providing opportunities\\nto debug where the reasoning path went wrong (although fully characterizing a model’s\\ncomputations that support an answer remains an open question).\\n3.Third, chain-of-thought reasoning can be used for tasks such as math word problems,\\ncommonsense reasoning, and symbolic manipulation, and is potentially applicable (at least\\nin principle) to any task that humans can solve via language.\\n4.Finally, chain-of-thought reasoning can be readily elicited in sufﬁciently large off-the-shelf\\nlanguage models simply by including examples of chain of thought sequences into the\\nexemplars of few-shot prompting.\\nIn empirical experiments, we will observe the utility of chain-of-thought prompting for arithmetic\\nreasoning (Section 3), commonsense reasoning (Section 4), and symbolic reasoning (Section 5).\\n3 Arithmetic Reasoning\\nWe begin by considering math word problems of the form in Figure 1, which measure the arithmetic\\nreasoning ability of language models. Though simple for humans, arithmetic reasoning is a task where\\nlanguage models often struggle (Hendrycks et al., 2021; Patel et al., 2021, inter alia ). Strikingly, chain-\\nof-thought prompting when used with the 540B parameter language model performs comparably with\\ntask-speciﬁc ﬁnetuned models on several tasks, even achieving new state of the art on the challenging\\nGSM8K benchmark (Cobbe et al., 2021).\\n3.1 Experimental Setup\\nWe explore chain-of-thought prompting for various language models on multiple benchmarks.\\nBenchmarks. We consider the following ﬁve math word problem benchmarks: (1)theGSM8K\\nbenchmark of math word problems (Cobbe et al., 2021), (2)theSV AMP dataset of math word\\nproblems with varying structures (Patel et al., 2021), (3)theASDiv dataset of diverse math word\\nproblems (Miao et al., 2020), (4)theAQuA dataset of algebraic word problems, and (5)theMA WPS\\nbenchmark (Koncel-Kedziorski et al., 2016). Example problems are given in Appendix Table 12.\\nStandard prompting. For the baseline, we consider standard few-shot prompting, popularized by\\nBrown et al. (2020), in which a language model is given in-context exemplars of input–output pairs\\nbefore outputting a prediction for a test-time example. Exemplars are formatted as questions and\\nanswers. The model gives the answer directly, as shown in Figure 1 (left).\\nChain-of-thought prompting. Our proposed approach is to augment each exemplar in few-shot\\nprompting with a chain of thought for an associated answer, as illustrated in Figure 1 (right). As most\\nof the datasets only have an evaluation split, we manually composed a set of eight few-shot exemplars\\nwith chains of thought for prompting—Figure 1 (right) shows one chain of thought exemplar, and the\\nfull set of exemplars is given in Appendix Table 20. (These particular exemplars did not undergo\\nprompt engineering; robustness is studied in Section 3.4 and Appendix A.2.) To investigate whether\\nchain-of-thought prompting in this form can successfully elicit successful reasoning across a range of\\n3'), Document(metadata={'author': '', 'creationdate': '2023-01-12T01:06:30+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2023-01-12T01:06:30+00:00', 'page': 2, 'page_label': '3', 'producer': 'pdfTeX-1.40.21', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': 'data2/test_paper_2.pdf', 'subject': '', 'title': '', 'total_pages': 43, 'trapped': '/False'}, page_content='language models can generate chains of thought if demonstrations of chain-of-thought reasoning are\\nprovided in the exemplars for few-shot prompting.\\nFigure 1 shows an example of a model producing a chain of thought to solve a math word problem\\nthat it would have otherwise gotten incorrect. The chain of thought in this case resembles a solution\\nand can interpreted as one, but we still opt to call it a chain of thought to better capture the idea that it\\nmimics a step-by-step thought process for arriving at the answer (and also, solutions/explanations\\ntypically come after the ﬁnal answer (Narang et al., 2020; Wiegreffe et al., 2022; Lampinen et al.,\\n2022, inter alia )).\\nChain-of-thought prompting has several attractive properties as an approach for facilitating reasoning\\nin language models.\\n1.First, chain of thought, in principle, allows models to decompose multi-step problems into\\nintermediate steps, which means that additional computation can be allocated to problems\\nthat require more reasoning steps.\\n2.Second, a chain of thought provides an interpretable window into the behavior of the model,\\nsuggesting how it might have arrived at a particular answer and providing opportunities\\nto debug where the reasoning path went wrong (although fully characterizing a model’s\\ncomputations that support an answer remains an open question).\\n3.Third, chain-of-thought reasoning can be used for tasks such as math word problems,\\ncommonsense reasoning, and symbolic manipulation, and is potentially applicable (at least\\nin principle) to any task that humans can solve via language.\\n4.Finally, chain-of-thought reasoning can be readily elicited in sufﬁciently large off-the-shelf\\nlanguage models simply by including examples of chain of thought sequences into the\\nexemplars of few-shot prompting.\\nIn empirical experiments, we will observe the utility of chain-of-thought prompting for arithmetic\\nreasoning (Section 3), commonsense reasoning (Section 4), and symbolic reasoning (Section 5).\\n3 Arithmetic Reasoning\\nWe begin by considering math word problems of the form in Figure 1, which measure the arithmetic\\nreasoning ability of language models. Though simple for humans, arithmetic reasoning is a task where\\nlanguage models often struggle (Hendrycks et al., 2021; Patel et al., 2021, inter alia ). Strikingly, chain-\\nof-thought prompting when used with the 540B parameter language model performs comparably with\\ntask-speciﬁc ﬁnetuned models on several tasks, even achieving new state of the art on the challenging\\nGSM8K benchmark (Cobbe et al., 2021).\\n3.1 Experimental Setup\\nWe explore chain-of-thought prompting for various language models on multiple benchmarks.\\nBenchmarks. We consider the following ﬁve math word problem benchmarks: (1)theGSM8K\\nbenchmark of math word problems (Cobbe et al., 2021), (2)theSV AMP dataset of math word\\nproblems with varying structures (Patel et al., 2021), (3)theASDiv dataset of diverse math word\\nproblems (Miao et al., 2020), (4)theAQuA dataset of algebraic word problems, and (5)theMA WPS\\nbenchmark (Koncel-Kedziorski et al., 2016). Example problems are given in Appendix Table 12.\\nStandard prompting. For the baseline, we consider standard few-shot prompting, popularized by\\nBrown et al. (2020), in which a language model is given in-context exemplars of input–output pairs\\nbefore outputting a prediction for a test-time example. Exemplars are formatted as questions and\\nanswers. The model gives the answer directly, as shown in Figure 1 (left).\\nChain-of-thought prompting. Our proposed approach is to augment each exemplar in few-shot\\nprompting with a chain of thought for an associated answer, as illustrated in Figure 1 (right). As most\\nof the datasets only have an evaluation split, we manually composed a set of eight few-shot exemplars\\nwith chains of thought for prompting—Figure 1 (right) shows one chain of thought exemplar, and the\\nfull set of exemplars is given in Appendix Table 20. (These particular exemplars did not undergo\\nprompt engineering; robustness is studied in Section 3.4 and Appendix A.2.) To investigate whether\\nchain-of-thought prompting in this form can successfully elicit successful reasoning across a range of\\n3'), Document(metadata={'author': '', 'creationdate': '2023-01-12T01:06:30+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2023-01-12T01:06:30+00:00', 'page': 8, 'page_label': '9', 'producer': 'pdfTeX-1.40.21', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': 'data2/test_paper_2.pdf', 'subject': '', 'title': '', 'total_pages': 43, 'trapped': '/False'}, page_content='experiments on commonsense reasoning underscored how the linguistic nature of chain-of-thought\\nreasoning makes it generally applicable (Section 4). Finally, we showed that for symbolic reasoning,\\nchain-of-thought prompting facilitates OOD generalization to longer sequence lengths (Section 5). In\\nall experiments, chain-of-thought reasoning is elicited simply by prompting an off-the-shelf language\\nmodel. No language models were ﬁnetuned in the process of writing this paper.\\nThe emergence of chain-of-thought reasoning as a result of model scale has been a prevailing theme\\n(Wei et al., 2022b). For many reasoning tasks where standard prompting has a ﬂat scaling curve, chain-\\nof-thought prompting leads to dramatically increasing scaling curves. Chain-of-thought prompting\\nappears to expand the set of tasks that large language models can perform successfully—in other\\nwords, our work underscores that standard prompting only provides a lower bound on the capabilities\\nof large language models. This observation likely raises more questions than it answers—for instance,\\nhow much more can we expect reasoning ability to improve with a further increase in model scale?\\nWhat other prompting methods might expand the range of tasks that language models can solve?\\nAs for limitations, we ﬁrst qualify that although chain of thought emulates the thought processes of\\nhuman reasoners, this does not answer whether the neural network is actually “reasoning,” which\\nwe leave as an open question. Second, although the cost of manually augmenting exemplars with\\nchains of thought is minimal in the few-shot setting, such annotation costs could be prohibitive for\\nﬁnetuning (though this could potentially be surmounted with synthetic data generation, or zero-shot\\ngeneralization). Third, there is no guarantee of correct reasoning paths, which can lead to both correct\\nand incorrect answers; improving factual generations of language models is an open direction for\\nfuture work (Rashkin et al., 2021; Ye and Durrett, 2022; Wiegreffe et al., 2022, inter alia ). Finally,\\nthe emergence of chain-of-thought reasoning only at large model scales makes it costly to serve in\\nreal-world applications; further research could explore how to induce reasoning in smaller models.\\n7 Related Work\\nThis work is inspired by many research areas, which we detail in an extended related work section\\n(Appendix C). Here we describe two directions and associated papers that are perhaps most relevant.\\nThe ﬁrst relevant direction is using intermediate steps to solve reasoning problems. Ling et al. (2017)\\npioneer the idea of using natural language rationales to solve math word problems through a series\\nof intermediate steps. Their work is a remarkable contrast to the literature using formal languages\\nto reason (Roy et al., 2015; Chiang and Chen, 2019; Amini et al., 2019; Chen et al., 2019). Cobbe\\net al. (2021) extend Ling et al. (2017) by creating a larger dataset and using it to ﬁnetune a pretrained\\nlanguage model rather than training a model from scratch. In the domain of program synthesis,\\nNye et al. (2021) leverage language models to predict the ﬁnal outputs of Python programs via\\nﬁrst line-to-line predicting the intermediate computational results, and show that their step-by-step\\nprediction method performs better than directly predicting the ﬁnal outputs.\\nNaturally, this paper also relates closely to the large body of recent work on prompting. Since the\\npopularization of few-shot prompting as given by Brown et al. (2020), several general approaches\\nhave improved the prompting ability of models, such as automatically learning prompts (Lester et al.,\\n2021) or giving models instructions describing a task (Wei et al., 2022a; Sanh et al., 2022; Ouyang\\net al., 2022). Whereas these approaches improve or augment the input part of the prompt (e.g.,\\ninstructions that are prepended to inputs), our work takes the orthogonal direction of augmenting the\\noutputs of language models with a chain of thought.\\n8 Conclusions\\nWe have explored chain-of-thought prompting as a simple and broadly applicable method for enhanc-\\ning reasoning in language models. Through experiments on arithmetic, symbolic, and commonsense\\nreasoning, we ﬁnd that chain-of-thought reasoning is an emergent property of model scale that allows\\nsufﬁciently large language models to perform reasoning tasks that otherwise have ﬂat scaling curves.\\nBroadening the range of reasoning tasks that language models can perform will hopefully inspire\\nfurther work on language-based approaches to reasoning.\\n9'), Document(metadata={'author': '', 'creationdate': '2023-01-12T01:06:30+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2023-01-12T01:06:30+00:00', 'page': 8, 'page_label': '9', 'producer': 'pdfTeX-1.40.21', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': 'data2/test_paper_2.pdf', 'subject': '', 'title': '', 'total_pages': 43, 'trapped': '/False'}, page_content='experiments on commonsense reasoning underscored how the linguistic nature of chain-of-thought\\nreasoning makes it generally applicable (Section 4). Finally, we showed that for symbolic reasoning,\\nchain-of-thought prompting facilitates OOD generalization to longer sequence lengths (Section 5). In\\nall experiments, chain-of-thought reasoning is elicited simply by prompting an off-the-shelf language\\nmodel. No language models were ﬁnetuned in the process of writing this paper.\\nThe emergence of chain-of-thought reasoning as a result of model scale has been a prevailing theme\\n(Wei et al., 2022b). For many reasoning tasks where standard prompting has a ﬂat scaling curve, chain-\\nof-thought prompting leads to dramatically increasing scaling curves. Chain-of-thought prompting\\nappears to expand the set of tasks that large language models can perform successfully—in other\\nwords, our work underscores that standard prompting only provides a lower bound on the capabilities\\nof large language models. This observation likely raises more questions than it answers—for instance,\\nhow much more can we expect reasoning ability to improve with a further increase in model scale?\\nWhat other prompting methods might expand the range of tasks that language models can solve?\\nAs for limitations, we ﬁrst qualify that although chain of thought emulates the thought processes of\\nhuman reasoners, this does not answer whether the neural network is actually “reasoning,” which\\nwe leave as an open question. Second, although the cost of manually augmenting exemplars with\\nchains of thought is minimal in the few-shot setting, such annotation costs could be prohibitive for\\nﬁnetuning (though this could potentially be surmounted with synthetic data generation, or zero-shot\\ngeneralization). Third, there is no guarantee of correct reasoning paths, which can lead to both correct\\nand incorrect answers; improving factual generations of language models is an open direction for\\nfuture work (Rashkin et al., 2021; Ye and Durrett, 2022; Wiegreffe et al., 2022, inter alia ). Finally,\\nthe emergence of chain-of-thought reasoning only at large model scales makes it costly to serve in\\nreal-world applications; further research could explore how to induce reasoning in smaller models.\\n7 Related Work\\nThis work is inspired by many research areas, which we detail in an extended related work section\\n(Appendix C). Here we describe two directions and associated papers that are perhaps most relevant.\\nThe ﬁrst relevant direction is using intermediate steps to solve reasoning problems. Ling et al. (2017)\\npioneer the idea of using natural language rationales to solve math word problems through a series\\nof intermediate steps. Their work is a remarkable contrast to the literature using formal languages\\nto reason (Roy et al., 2015; Chiang and Chen, 2019; Amini et al., 2019; Chen et al., 2019). Cobbe\\net al. (2021) extend Ling et al. (2017) by creating a larger dataset and using it to ﬁnetune a pretrained\\nlanguage model rather than training a model from scratch. In the domain of program synthesis,\\nNye et al. (2021) leverage language models to predict the ﬁnal outputs of Python programs via\\nﬁrst line-to-line predicting the intermediate computational results, and show that their step-by-step\\nprediction method performs better than directly predicting the ﬁnal outputs.\\nNaturally, this paper also relates closely to the large body of recent work on prompting. Since the\\npopularization of few-shot prompting as given by Brown et al. (2020), several general approaches\\nhave improved the prompting ability of models, such as automatically learning prompts (Lester et al.,\\n2021) or giving models instructions describing a task (Wei et al., 2022a; Sanh et al., 2022; Ouyang\\net al., 2022). Whereas these approaches improve or augment the input part of the prompt (e.g.,\\ninstructions that are prepended to inputs), our work takes the orthogonal direction of augmenting the\\noutputs of language models with a chain of thought.\\n8 Conclusions\\nWe have explored chain-of-thought prompting as a simple and broadly applicable method for enhanc-\\ning reasoning in language models. Through experiments on arithmetic, symbolic, and commonsense\\nreasoning, we ﬁnd that chain-of-thought reasoning is an emergent property of model scale that allows\\nsufﬁciently large language models to perform reasoning tasks that otherwise have ﬂat scaling curves.\\nBroadening the range of reasoning tasks that language models can perform will hopefully inspire\\nfurther work on language-based approaches to reasoning.\\n9')]\n"
     ]
    }
   ],
   "source": [
    "query = \"What does 'chain of thought' mean?\"\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\")\n",
    "search_result = retriever.get_relevant_documents(query)\n",
    "print(search_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`similarity_score_threshold` 는 유사도 기반 검색에서 `score_threshold` 이상인 결과만 반환합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📌 [문서 메타데이터]: {'author': '', 'creationdate': '2023-01-12T01:06:30+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2023-01-12T01:06:30+00:00', 'page': 8, 'page_label': '9', 'producer': 'pdfTeX-1.40.21', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': 'data2/test_paper_2.pdf', 'subject': '', 'title': '', 'total_pages': 43, 'trapped': '/False'}\n",
      "📄 [문서 내용]: experiments on commonsense reasoning underscored how the linguistic nature of chain-of-thought\n",
      "reasoning makes it generally applicable (Section 4). Finally, we showed that for symbolic reasoning,\n",
      "chain-of-thought prompting facilitates OOD generalization to longer sequence lengths (Section 5). In\n",
      "all experiments, chain-of-thought reasoning is elicited simply by prompting an off-the-shelf language\n",
      "model. No language models were ﬁnetuned in the process of writing this paper.\n",
      "The emergence of chain-\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "query = \"What does 'chain-of-thought' mean?\"\n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",  # `k` 값을 적용할 수 있도록 설정\n",
    "    search_kwargs={\"k\": 1}  # 최대 1개의 문서만 반환하도록 제한\n",
    ")\n",
    "\n",
    "search_result = retriever.get_relevant_documents(query)\n",
    "\n",
    "# 결과 출력 (최대 1개만 출력)\n",
    "if not search_result:\n",
    "    print(\"❌ No relevant documents found.\")\n",
    "else:\n",
    "    doc = search_result[0]  # 가장 유사한 문서 1개만 가져옴\n",
    "    print(f\"\\n📌 [문서 메타데이터]: {doc.metadata}\")\n",
    "    print(f\"📄 [문서 내용]: {doc.page_content[:500]}\")  # 500자까지만 출력\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No relevant docs were retrieved using the relevance score threshold 0.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "❌ No relevant documents found.\n"
     ]
    }
   ],
   "source": [
    "query = \"What does 'chain-of-thought' mean?\"\n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.8, \"k\": 1}\n",
    ")\n",
    "search_result = retriever.get_relevant_documents(query)\n",
    "print(search_result)\n",
    "if not search_result:\n",
    "    print(\"❌ No relevant documents found.\")\n",
    "else:\n",
    "    doc = search_result[0]  # 가장 유사한 문서 1개만 가져옴\n",
    "    print(f\"\\n📌 [문서 메타데이터]: {doc.metadata}\")\n",
    "    print(f\"📄 [문서 내용]: {doc.page_content}\")  # 500자까지만 출력\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`maximum marginal search result` 를 사용하여 검색합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📌 [문서 메타데이터]: {'author': '', 'creationdate': '2023-01-12T01:06:30+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2023-01-12T01:06:30+00:00', 'page': 8, 'page_label': '9', 'producer': 'pdfTeX-1.40.21', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': 'data2/test_paper_2.pdf', 'subject': '', 'title': '', 'total_pages': 43, 'trapped': '/False'}\n",
      "📄 [문서 내용]: experiments on commonsense reasoning underscored how the linguistic nature of chain-of-thought\n",
      "reasoning makes it generally applicable (Section 4). Finally, we showed that for symbolic reasoning,\n",
      "chain-of-thought prompting facilitates OOD generalization to longer sequence lengths (Section 5). In\n",
      "all experiments, chain-of-thought reasoning is elicited simply by prompting an off-the-shelf language\n",
      "model. No language models were ﬁnetuned in the process of writing this paper.\n",
      "The emergence of chain-\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "query = \"What does 'chain-of-thought' mean?\"\n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",  # `k` 값을 적용할 수 있도록 설정\n",
    "    search_kwargs={\"k\": 2}  # 최대 2개의 문서만 반환하도록 제한\n",
    ")\n",
    "\n",
    "search_result = retriever.get_relevant_documents(query)\n",
    "\n",
    "# 결과 출력 (최대 1개만 출력)\n",
    "if not search_result:\n",
    "    print(\"❌ No relevant documents found.\")\n",
    "else:\n",
    "    doc = search_result[0]  # 가장 유사한 문서 1개만 가져옴\n",
    "    print(f\"\\n📌 [문서 메타데이터]: {doc.metadata}\")\n",
    "    print(f\"📄 [문서 내용]: {doc.page_content[:500]}\")  # 500자까지만 출력\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다양한 쿼리 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "query = \"\"\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectorstore.as_retriever(), llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logging for the queries\n",
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. Can you explain the concept of chain-of-thought prompting?', '2. How does chain-of-thought prompting work?', '3. What are the key aspects of chain-of-thought prompting?']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is chain-of-thought prompting?\"\n",
    "unique_docs = retriever_from_llm.get_relevant_documents(query=question)\n",
    "len(unique_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Retrievers initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# ✅ `page_content`만 추출해서 문자열 리스트로 변환*\n",
    "doc_texts = [d.page_content for d in docs]  # `BM25Retriever`와 `FAISS`에서 사용 가능\n",
    "\n",
    "# ✅ BM25 Retriever 초기화\n",
    "bm25_retriever = BM25Retriever.from_texts(doc_texts)  \n",
    "bm25_retriever.k = 2  # 검색할 최대 문서 개수\n",
    "\n",
    "# ✅ FAISS Retriever 초기화\n",
    "faiss_vectorstore = FAISS.from_texts(doc_texts, OpenAIEmbeddings())\n",
    "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# ✅ Ensemble Retriever 초기화 (BM25 + FAISS)\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, faiss_retriever], \n",
    "    weights=[0.5, 0.5]  # BM25와 FAISS에 동일한 가중치 적용\n",
    ")\n",
    "\n",
    "print(\"✅ Retrievers initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print(docs):\n",
    "    for i, doc in enumerate(docs):\n",
    "        print(f\"[{i+1}] {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Query]\n",
      "chain-of-thought 개념을 알려줘\n",
      "\n",
      "[BM25 Retriever]\n",
      "[1] 0204060GSM8K\n",
      "solve rate (%)LaMDA GPT PaLMStandard prompting\n",
      "Chain-of-thought prompting\n",
      "Prior supervised best\n",
      "020406080SV AMP\n",
      "solve rate (%)\n",
      "0.4 81370255075100MAWPS\n",
      "solve rate (%)\n",
      "0.4 7175 862540\n",
      "Model scale (# parameters in billions)\n",
      "Figure 4: Chain-of-thought prompting enables\n",
      "large language models to solve challenging math\n",
      "problems. Notably, chain-of-thought reasoning\n",
      "is an emergent ability of increasing model scale.\n",
      "Prior best numbers are from Cobbe et al. (2021)\n",
      "for GSM8K, Jie et al. (2022) for SV AMP, and Lan\n",
      "et al. (2021) for MAWPS.Second, chain-of-thought prompting has larger\n",
      "performance gains for more-complicated prob-\n",
      "lems. For instance, for GSM8K (the dataset\n",
      "with the lowest baseline performance), perfor-\n",
      "mance more than doubled for the largest GPT\n",
      "and PaLM models. On the other hand, for Sin-\n",
      "gleOp, the easiest subset of MAWPS which only\n",
      "requires a single step to solve, performance im-\n",
      "provements were either negative or very small\n",
      "(see Appendix Table 3).\n",
      "Third, chain-of-thought prompting via GPT-3\n",
      "175B and PaLM 540B compares favorably to\n",
      "prior state of the art, which typically ﬁnetunes a\n",
      "task-speciﬁc model on a labeled training dataset.\n",
      "Figure 4 shows how PaLM 540B uses chain-of-\n",
      "thought prompting to achieve new state of the art\n",
      "on GSM8K, SV AMP, and MAWPS (though note\n",
      "that standard prompting already passed the prior\n",
      "best for SV AMP). On the other two datasets,\n",
      "AQuA and ASDiv, PaLM with chain-of-thought\n",
      "prompting reaches within 2% of the state of the\n",
      "art (Appendix Table 2).\n",
      "To better understand why chain-of-thought\n",
      "prompting works, we manually examined model-\n",
      "generated chains of thought by LaMDA 137B\n",
      "for GSM8K. Of 50 random examples where the\n",
      "model returned the correct ﬁnal answer, all of\n",
      "the generated chains of thought were also log-\n",
      "ically and mathematically correct except two\n",
      "that coincidentally arrived at the correct answer\n",
      "(see Appendix D.1, and Table 8 for examples\n",
      "of correct model-generated chains of thought).\n",
      "We also randomly examined 50 random sam-\n",
      "ples for which the model gave the wrong answer.\n",
      "The summary of this analysis is that 46% of the\n",
      "chains of thought were almost correct, barring\n",
      "minor mistakes (calculator error, symbol map-\n",
      "ping error, or one reasoning step missing), and that the other 54% of the chains of thought had major\n",
      "errors in semantic understanding or coherence (see Appendix D.2). To provide a small insight into\n",
      "why scaling improves chain-of-thought reasoning ability, we performed a similar analysis of errors\n",
      "made by PaLM 62B and whether those errors were ﬁxed by scaling to PaLM 540B. The summary\n",
      "is that scaling PaLM to 540B ﬁxes a large portion of one-step missing and semantic understanding\n",
      "errors in the 62B model (see Appendix A.1).\n",
      "3.3 Ablation Study\n",
      "The observed beneﬁts of using chain-of-thought prompting raises the natural question of whether the\n",
      "same performance improvements can be conferred via other types of prompting. Figure 5 shows an\n",
      "ablation study with three variations of chain of thought described below.\n",
      "Equation only. One reason for why chain-of-thought prompting might help is that it produces the\n",
      "mathematical equation to be evaluated, and so we test a variation where the model is prompted\n",
      "to output only a mathematical equation before giving the answer. Figure 5 shows that equation\n",
      "only prompting does not help much for GSM8K, which implies that the semantics of the questions\n",
      "in GSM8K are too challenging to directly translate into an equation without the natural language\n",
      "reasoning steps in chain of thought. For datasets of one-step or two-step problems, however, we ﬁnd\n",
      "that equation only prompting does improve performance, since the equation can be easily derived\n",
      "from the question (see Appendix Table 6).\n",
      "5\n",
      "[2] experiments on commonsense reasoning underscored how the linguistic nature of chain-of-thought\n",
      "reasoning makes it generally applicable (Section 4). Finally, we showed that for symbolic reasoning,\n",
      "chain-of-thought prompting facilitates OOD generalization to longer sequence lengths (Section 5). In\n",
      "all experiments, chain-of-thought reasoning is elicited simply by prompting an off-the-shelf language\n",
      "model. No language models were ﬁnetuned in the process of writing this paper.\n",
      "The emergence of chain-of-thought reasoning as a result of model scale has been a prevailing theme\n",
      "(Wei et al., 2022b). For many reasoning tasks where standard prompting has a ﬂat scaling curve, chain-\n",
      "of-thought prompting leads to dramatically increasing scaling curves. Chain-of-thought prompting\n",
      "appears to expand the set of tasks that large language models can perform successfully—in other\n",
      "words, our work underscores that standard prompting only provides a lower bound on the capabilities\n",
      "of large language models. This observation likely raises more questions than it answers—for instance,\n",
      "how much more can we expect reasoning ability to improve with a further increase in model scale?\n",
      "What other prompting methods might expand the range of tasks that language models can solve?\n",
      "As for limitations, we ﬁrst qualify that although chain of thought emulates the thought processes of\n",
      "human reasoners, this does not answer whether the neural network is actually “reasoning,” which\n",
      "we leave as an open question. Second, although the cost of manually augmenting exemplars with\n",
      "chains of thought is minimal in the few-shot setting, such annotation costs could be prohibitive for\n",
      "ﬁnetuning (though this could potentially be surmounted with synthetic data generation, or zero-shot\n",
      "generalization). Third, there is no guarantee of correct reasoning paths, which can lead to both correct\n",
      "and incorrect answers; improving factual generations of language models is an open direction for\n",
      "future work (Rashkin et al., 2021; Ye and Durrett, 2022; Wiegreffe et al., 2022, inter alia ). Finally,\n",
      "the emergence of chain-of-thought reasoning only at large model scales makes it costly to serve in\n",
      "real-world applications; further research could explore how to induce reasoning in smaller models.\n",
      "7 Related Work\n",
      "This work is inspired by many research areas, which we detail in an extended related work section\n",
      "(Appendix C). Here we describe two directions and associated papers that are perhaps most relevant.\n",
      "The ﬁrst relevant direction is using intermediate steps to solve reasoning problems. Ling et al. (2017)\n",
      "pioneer the idea of using natural language rationales to solve math word problems through a series\n",
      "of intermediate steps. Their work is a remarkable contrast to the literature using formal languages\n",
      "to reason (Roy et al., 2015; Chiang and Chen, 2019; Amini et al., 2019; Chen et al., 2019). Cobbe\n",
      "et al. (2021) extend Ling et al. (2017) by creating a larger dataset and using it to ﬁnetune a pretrained\n",
      "language model rather than training a model from scratch. In the domain of program synthesis,\n",
      "Nye et al. (2021) leverage language models to predict the ﬁnal outputs of Python programs via\n",
      "ﬁrst line-to-line predicting the intermediate computational results, and show that their step-by-step\n",
      "prediction method performs better than directly predicting the ﬁnal outputs.\n",
      "Naturally, this paper also relates closely to the large body of recent work on prompting. Since the\n",
      "popularization of few-shot prompting as given by Brown et al. (2020), several general approaches\n",
      "have improved the prompting ability of models, such as automatically learning prompts (Lester et al.,\n",
      "2021) or giving models instructions describing a task (Wei et al., 2022a; Sanh et al., 2022; Ouyang\n",
      "et al., 2022). Whereas these approaches improve or augment the input part of the prompt (e.g.,\n",
      "instructions that are prepended to inputs), our work takes the orthogonal direction of augmenting the\n",
      "outputs of language models with a chain of thought.\n",
      "8 Conclusions\n",
      "We have explored chain-of-thought prompting as a simple and broadly applicable method for enhanc-\n",
      "ing reasoning in language models. Through experiments on arithmetic, symbolic, and commonsense\n",
      "reasoning, we ﬁnd that chain-of-thought reasoning is an emergent property of model scale that allows\n",
      "sufﬁciently large language models to perform reasoning tasks that otherwise have ﬂat scaling curves.\n",
      "Broadening the range of reasoning tasks that language models can perform will hopefully inspire\n",
      "further work on language-based approaches to reasoning.\n",
      "9\n",
      "============================================================\n",
      "[FAISS Retriever]\n",
      "[1] language models can generate chains of thought if demonstrations of chain-of-thought reasoning are\n",
      "provided in the exemplars for few-shot prompting.\n",
      "Figure 1 shows an example of a model producing a chain of thought to solve a math word problem\n",
      "that it would have otherwise gotten incorrect. The chain of thought in this case resembles a solution\n",
      "and can interpreted as one, but we still opt to call it a chain of thought to better capture the idea that it\n",
      "mimics a step-by-step thought process for arriving at the answer (and also, solutions/explanations\n",
      "typically come after the ﬁnal answer (Narang et al., 2020; Wiegreffe et al., 2022; Lampinen et al.,\n",
      "2022, inter alia )).\n",
      "Chain-of-thought prompting has several attractive properties as an approach for facilitating reasoning\n",
      "in language models.\n",
      "1.First, chain of thought, in principle, allows models to decompose multi-step problems into\n",
      "intermediate steps, which means that additional computation can be allocated to problems\n",
      "that require more reasoning steps.\n",
      "2.Second, a chain of thought provides an interpretable window into the behavior of the model,\n",
      "suggesting how it might have arrived at a particular answer and providing opportunities\n",
      "to debug where the reasoning path went wrong (although fully characterizing a model’s\n",
      "computations that support an answer remains an open question).\n",
      "3.Third, chain-of-thought reasoning can be used for tasks such as math word problems,\n",
      "commonsense reasoning, and symbolic manipulation, and is potentially applicable (at least\n",
      "in principle) to any task that humans can solve via language.\n",
      "4.Finally, chain-of-thought reasoning can be readily elicited in sufﬁciently large off-the-shelf\n",
      "language models simply by including examples of chain of thought sequences into the\n",
      "exemplars of few-shot prompting.\n",
      "In empirical experiments, we will observe the utility of chain-of-thought prompting for arithmetic\n",
      "reasoning (Section 3), commonsense reasoning (Section 4), and symbolic reasoning (Section 5).\n",
      "3 Arithmetic Reasoning\n",
      "We begin by considering math word problems of the form in Figure 1, which measure the arithmetic\n",
      "reasoning ability of language models. Though simple for humans, arithmetic reasoning is a task where\n",
      "language models often struggle (Hendrycks et al., 2021; Patel et al., 2021, inter alia ). Strikingly, chain-\n",
      "of-thought prompting when used with the 540B parameter language model performs comparably with\n",
      "task-speciﬁc ﬁnetuned models on several tasks, even achieving new state of the art on the challenging\n",
      "GSM8K benchmark (Cobbe et al., 2021).\n",
      "3.1 Experimental Setup\n",
      "We explore chain-of-thought prompting for various language models on multiple benchmarks.\n",
      "Benchmarks. We consider the following ﬁve math word problem benchmarks: (1)theGSM8K\n",
      "benchmark of math word problems (Cobbe et al., 2021), (2)theSV AMP dataset of math word\n",
      "problems with varying structures (Patel et al., 2021), (3)theASDiv dataset of diverse math word\n",
      "problems (Miao et al., 2020), (4)theAQuA dataset of algebraic word problems, and (5)theMA WPS\n",
      "benchmark (Koncel-Kedziorski et al., 2016). Example problems are given in Appendix Table 12.\n",
      "Standard prompting. For the baseline, we consider standard few-shot prompting, popularized by\n",
      "Brown et al. (2020), in which a language model is given in-context exemplars of input–output pairs\n",
      "before outputting a prediction for a test-time example. Exemplars are formatted as questions and\n",
      "answers. The model gives the answer directly, as shown in Figure 1 (left).\n",
      "Chain-of-thought prompting. Our proposed approach is to augment each exemplar in few-shot\n",
      "prompting with a chain of thought for an associated answer, as illustrated in Figure 1 (right). As most\n",
      "of the datasets only have an evaluation split, we manually composed a set of eight few-shot exemplars\n",
      "with chains of thought for prompting—Figure 1 (right) shows one chain of thought exemplar, and the\n",
      "full set of exemplars is given in Appendix Table 20. (These particular exemplars did not undergo\n",
      "prompt engineering; robustness is studied in Section 3.4 and Appendix A.2.) To investigate whether\n",
      "chain-of-thought prompting in this form can successfully elicit successful reasoning across a range of\n",
      "3\n",
      "[2] C Extended Related Work\n",
      "Chain-of-thought prompting is a general approach that is inspired by several prior directions: prompt-\n",
      "ing, natural language explanations, program synthesis/execution, numeric and logical reasoning, and\n",
      "intermediate language steps.\n",
      "C.1 Prompting\n",
      "The recent success of large-scale language models has led to growing interest in improving their\n",
      "capability to perform tasks via prompting (Brown et al. (2020), and see Liu et al. (2021) for a\n",
      "survey). This paper falls in the category of general prompting approaches, whereby input prompts are\n",
      "optimized to allow a single large language model to better perform a variety of tasks (Li and Liang,\n",
      "2021; Lester et al., 2021; Reif et al., 2022, inter alia ).\n",
      "One recent line of work aims to improve the ability of language models to perform a task by providing\n",
      "instructions that describe the task (Raffel et al., 2020; Wei et al., 2022a; Ouyang et al., 2022; Sanh\n",
      "et al., 2022; Wang et al., 2022b). This line of work is related because it also augments input–output\n",
      "pairs with meta-data. But whereas an instruction augments the input to a task (instructions are typically\n",
      "prepended to the inputs), chain-of-thought prompting augments the outputs of language models.\n",
      "Another related direction is sequentially combining the outputs of language models; human–computer\n",
      "interaction (HCI) work (Wu et al., 2022a,b) has shown that combining sequential generations of\n",
      "language models improves task outcomes in a 20-person user study.\n",
      "C.2 Natural language explanations\n",
      "Another closely related direction uses natural language explanations (NLEs), often with the goal of\n",
      "improving model interpretability (Zhou et al., 2020; Wiegreffe and Marasovi ´c, 2021, inter alia ). That\n",
      "line of work typically focuses on natural language inference (Camburu et al., 2018; Yordanov et al.,\n",
      "2021; Bostrom et al., 2021), and produces explanations either simultaneously to or after the ﬁnal\n",
      "prediction (Narang et al., 2020; Majumder et al., 2021; Wiegreffe et al., 2021, 2022). By contrast,\n",
      "the chain of thought processing considered in this paper occurs before the ﬁnal answer. And while\n",
      "NLE aims mostly to improve neural network interpretability (Rajagopal et al., 2021), the goal of\n",
      "chain-of-thought prompting is to allow models to decompose multi-hop reasoning tasks into multiple\n",
      "steps—interpretability is just a side effect. Marasovi ´c et al. (2022) show that prompt-based ﬁnetuning\n",
      "with NLE improves NLI and classiﬁcation performance, though they largely focus on evaluating\n",
      "explanation plausibility. In comparison, our work focuses on a range of arithmetic, commonsense,\n",
      "and symbolic tasks that require multi-hop reasoning.\n",
      "C.3 Program synthesis and execution\n",
      "Using intermediate reasoning steps has a long history in program synthesis and execution (Zaremba\n",
      "and Sutskever, 2014, inter alia ). Recent work along in this direction has included a number of\n",
      "architectural innovations (Cai et al., 2017; Dong et al., 2019; Yan et al., 2020), as well as the use of\n",
      "large language models (Chen et al., 2021; Austin et al., 2021). The program execution work closest to\n",
      "ours is perhaps Nye et al. (2021), which show that large language models can perform up to 10-digit\n",
      "addition, evaluate polynomials, and execute python programs. Whereas generating a program and\n",
      "then executing it can be viewed as a type of reasoning, our work generalizes such domain-speciﬁc\n",
      "primitives to natural language, which is open-domain and relevant to any text-to-text NLP task in\n",
      "principle.\n",
      "C.4 Numeric and logical reasoning\n",
      "Numeric and logical reasoning has been a long-studied task in machine learning and natural language\n",
      "processing (Lev et al., 2004, inter alia ). Recent work has also aimed to inject numeric reasoning\n",
      "abilities in language models in various ways, such as augmenting BERT with a predeﬁned set of\n",
      "executable operations (Andor et al., 2019), including a graph neural network (Ran et al., 2019), and\n",
      "using specialized training procedures (Pi˛ ekos et al., 2021). Another line of work aims to enable\n",
      "language models to perform logical or formal reasoning, often by verablizing the rules in natural\n",
      "language formal rules using language (Clark et al., 2020; Saeed et al., 2021; Liang et al., 2021).\n",
      "24\n",
      "============================================================\n",
      "[Ensemble Retriever]\n",
      "[1] 0204060GSM8K\n",
      "solve rate (%)LaMDA GPT PaLMStandard prompting\n",
      "Chain-of-thought prompting\n",
      "Prior supervised best\n",
      "020406080SV AMP\n",
      "solve rate (%)\n",
      "0.4 81370255075100MAWPS\n",
      "solve rate (%)\n",
      "0.4 7175 862540\n",
      "Model scale (# parameters in billions)\n",
      "Figure 4: Chain-of-thought prompting enables\n",
      "large language models to solve challenging math\n",
      "problems. Notably, chain-of-thought reasoning\n",
      "is an emergent ability of increasing model scale.\n",
      "Prior best numbers are from Cobbe et al. (2021)\n",
      "for GSM8K, Jie et al. (2022) for SV AMP, and Lan\n",
      "et al. (2021) for MAWPS.Second, chain-of-thought prompting has larger\n",
      "performance gains for more-complicated prob-\n",
      "lems. For instance, for GSM8K (the dataset\n",
      "with the lowest baseline performance), perfor-\n",
      "mance more than doubled for the largest GPT\n",
      "and PaLM models. On the other hand, for Sin-\n",
      "gleOp, the easiest subset of MAWPS which only\n",
      "requires a single step to solve, performance im-\n",
      "provements were either negative or very small\n",
      "(see Appendix Table 3).\n",
      "Third, chain-of-thought prompting via GPT-3\n",
      "175B and PaLM 540B compares favorably to\n",
      "prior state of the art, which typically ﬁnetunes a\n",
      "task-speciﬁc model on a labeled training dataset.\n",
      "Figure 4 shows how PaLM 540B uses chain-of-\n",
      "thought prompting to achieve new state of the art\n",
      "on GSM8K, SV AMP, and MAWPS (though note\n",
      "that standard prompting already passed the prior\n",
      "best for SV AMP). On the other two datasets,\n",
      "AQuA and ASDiv, PaLM with chain-of-thought\n",
      "prompting reaches within 2% of the state of the\n",
      "art (Appendix Table 2).\n",
      "To better understand why chain-of-thought\n",
      "prompting works, we manually examined model-\n",
      "generated chains of thought by LaMDA 137B\n",
      "for GSM8K. Of 50 random examples where the\n",
      "model returned the correct ﬁnal answer, all of\n",
      "the generated chains of thought were also log-\n",
      "ically and mathematically correct except two\n",
      "that coincidentally arrived at the correct answer\n",
      "(see Appendix D.1, and Table 8 for examples\n",
      "of correct model-generated chains of thought).\n",
      "We also randomly examined 50 random sam-\n",
      "ples for which the model gave the wrong answer.\n",
      "The summary of this analysis is that 46% of the\n",
      "chains of thought were almost correct, barring\n",
      "minor mistakes (calculator error, symbol map-\n",
      "ping error, or one reasoning step missing), and that the other 54% of the chains of thought had major\n",
      "errors in semantic understanding or coherence (see Appendix D.2). To provide a small insight into\n",
      "why scaling improves chain-of-thought reasoning ability, we performed a similar analysis of errors\n",
      "made by PaLM 62B and whether those errors were ﬁxed by scaling to PaLM 540B. The summary\n",
      "is that scaling PaLM to 540B ﬁxes a large portion of one-step missing and semantic understanding\n",
      "errors in the 62B model (see Appendix A.1).\n",
      "3.3 Ablation Study\n",
      "The observed beneﬁts of using chain-of-thought prompting raises the natural question of whether the\n",
      "same performance improvements can be conferred via other types of prompting. Figure 5 shows an\n",
      "ablation study with three variations of chain of thought described below.\n",
      "Equation only. One reason for why chain-of-thought prompting might help is that it produces the\n",
      "mathematical equation to be evaluated, and so we test a variation where the model is prompted\n",
      "to output only a mathematical equation before giving the answer. Figure 5 shows that equation\n",
      "only prompting does not help much for GSM8K, which implies that the semantics of the questions\n",
      "in GSM8K are too challenging to directly translate into an equation without the natural language\n",
      "reasoning steps in chain of thought. For datasets of one-step or two-step problems, however, we ﬁnd\n",
      "that equation only prompting does improve performance, since the equation can be easily derived\n",
      "from the question (see Appendix Table 6).\n",
      "5\n",
      "[2] language models can generate chains of thought if demonstrations of chain-of-thought reasoning are\n",
      "provided in the exemplars for few-shot prompting.\n",
      "Figure 1 shows an example of a model producing a chain of thought to solve a math word problem\n",
      "that it would have otherwise gotten incorrect. The chain of thought in this case resembles a solution\n",
      "and can interpreted as one, but we still opt to call it a chain of thought to better capture the idea that it\n",
      "mimics a step-by-step thought process for arriving at the answer (and also, solutions/explanations\n",
      "typically come after the ﬁnal answer (Narang et al., 2020; Wiegreffe et al., 2022; Lampinen et al.,\n",
      "2022, inter alia )).\n",
      "Chain-of-thought prompting has several attractive properties as an approach for facilitating reasoning\n",
      "in language models.\n",
      "1.First, chain of thought, in principle, allows models to decompose multi-step problems into\n",
      "intermediate steps, which means that additional computation can be allocated to problems\n",
      "that require more reasoning steps.\n",
      "2.Second, a chain of thought provides an interpretable window into the behavior of the model,\n",
      "suggesting how it might have arrived at a particular answer and providing opportunities\n",
      "to debug where the reasoning path went wrong (although fully characterizing a model’s\n",
      "computations that support an answer remains an open question).\n",
      "3.Third, chain-of-thought reasoning can be used for tasks such as math word problems,\n",
      "commonsense reasoning, and symbolic manipulation, and is potentially applicable (at least\n",
      "in principle) to any task that humans can solve via language.\n",
      "4.Finally, chain-of-thought reasoning can be readily elicited in sufﬁciently large off-the-shelf\n",
      "language models simply by including examples of chain of thought sequences into the\n",
      "exemplars of few-shot prompting.\n",
      "In empirical experiments, we will observe the utility of chain-of-thought prompting for arithmetic\n",
      "reasoning (Section 3), commonsense reasoning (Section 4), and symbolic reasoning (Section 5).\n",
      "3 Arithmetic Reasoning\n",
      "We begin by considering math word problems of the form in Figure 1, which measure the arithmetic\n",
      "reasoning ability of language models. Though simple for humans, arithmetic reasoning is a task where\n",
      "language models often struggle (Hendrycks et al., 2021; Patel et al., 2021, inter alia ). Strikingly, chain-\n",
      "of-thought prompting when used with the 540B parameter language model performs comparably with\n",
      "task-speciﬁc ﬁnetuned models on several tasks, even achieving new state of the art on the challenging\n",
      "GSM8K benchmark (Cobbe et al., 2021).\n",
      "3.1 Experimental Setup\n",
      "We explore chain-of-thought prompting for various language models on multiple benchmarks.\n",
      "Benchmarks. We consider the following ﬁve math word problem benchmarks: (1)theGSM8K\n",
      "benchmark of math word problems (Cobbe et al., 2021), (2)theSV AMP dataset of math word\n",
      "problems with varying structures (Patel et al., 2021), (3)theASDiv dataset of diverse math word\n",
      "problems (Miao et al., 2020), (4)theAQuA dataset of algebraic word problems, and (5)theMA WPS\n",
      "benchmark (Koncel-Kedziorski et al., 2016). Example problems are given in Appendix Table 12.\n",
      "Standard prompting. For the baseline, we consider standard few-shot prompting, popularized by\n",
      "Brown et al. (2020), in which a language model is given in-context exemplars of input–output pairs\n",
      "before outputting a prediction for a test-time example. Exemplars are formatted as questions and\n",
      "answers. The model gives the answer directly, as shown in Figure 1 (left).\n",
      "Chain-of-thought prompting. Our proposed approach is to augment each exemplar in few-shot\n",
      "prompting with a chain of thought for an associated answer, as illustrated in Figure 1 (right). As most\n",
      "of the datasets only have an evaluation split, we manually composed a set of eight few-shot exemplars\n",
      "with chains of thought for prompting—Figure 1 (right) shows one chain of thought exemplar, and the\n",
      "full set of exemplars is given in Appendix Table 20. (These particular exemplars did not undergo\n",
      "prompt engineering; robustness is studied in Section 3.4 and Appendix A.2.) To investigate whether\n",
      "chain-of-thought prompting in this form can successfully elicit successful reasoning across a range of\n",
      "3\n",
      "[3] experiments on commonsense reasoning underscored how the linguistic nature of chain-of-thought\n",
      "reasoning makes it generally applicable (Section 4). Finally, we showed that for symbolic reasoning,\n",
      "chain-of-thought prompting facilitates OOD generalization to longer sequence lengths (Section 5). In\n",
      "all experiments, chain-of-thought reasoning is elicited simply by prompting an off-the-shelf language\n",
      "model. No language models were ﬁnetuned in the process of writing this paper.\n",
      "The emergence of chain-of-thought reasoning as a result of model scale has been a prevailing theme\n",
      "(Wei et al., 2022b). For many reasoning tasks where standard prompting has a ﬂat scaling curve, chain-\n",
      "of-thought prompting leads to dramatically increasing scaling curves. Chain-of-thought prompting\n",
      "appears to expand the set of tasks that large language models can perform successfully—in other\n",
      "words, our work underscores that standard prompting only provides a lower bound on the capabilities\n",
      "of large language models. This observation likely raises more questions than it answers—for instance,\n",
      "how much more can we expect reasoning ability to improve with a further increase in model scale?\n",
      "What other prompting methods might expand the range of tasks that language models can solve?\n",
      "As for limitations, we ﬁrst qualify that although chain of thought emulates the thought processes of\n",
      "human reasoners, this does not answer whether the neural network is actually “reasoning,” which\n",
      "we leave as an open question. Second, although the cost of manually augmenting exemplars with\n",
      "chains of thought is minimal in the few-shot setting, such annotation costs could be prohibitive for\n",
      "ﬁnetuning (though this could potentially be surmounted with synthetic data generation, or zero-shot\n",
      "generalization). Third, there is no guarantee of correct reasoning paths, which can lead to both correct\n",
      "and incorrect answers; improving factual generations of language models is an open direction for\n",
      "future work (Rashkin et al., 2021; Ye and Durrett, 2022; Wiegreffe et al., 2022, inter alia ). Finally,\n",
      "the emergence of chain-of-thought reasoning only at large model scales makes it costly to serve in\n",
      "real-world applications; further research could explore how to induce reasoning in smaller models.\n",
      "7 Related Work\n",
      "This work is inspired by many research areas, which we detail in an extended related work section\n",
      "(Appendix C). Here we describe two directions and associated papers that are perhaps most relevant.\n",
      "The ﬁrst relevant direction is using intermediate steps to solve reasoning problems. Ling et al. (2017)\n",
      "pioneer the idea of using natural language rationales to solve math word problems through a series\n",
      "of intermediate steps. Their work is a remarkable contrast to the literature using formal languages\n",
      "to reason (Roy et al., 2015; Chiang and Chen, 2019; Amini et al., 2019; Chen et al., 2019). Cobbe\n",
      "et al. (2021) extend Ling et al. (2017) by creating a larger dataset and using it to ﬁnetune a pretrained\n",
      "language model rather than training a model from scratch. In the domain of program synthesis,\n",
      "Nye et al. (2021) leverage language models to predict the ﬁnal outputs of Python programs via\n",
      "ﬁrst line-to-line predicting the intermediate computational results, and show that their step-by-step\n",
      "prediction method performs better than directly predicting the ﬁnal outputs.\n",
      "Naturally, this paper also relates closely to the large body of recent work on prompting. Since the\n",
      "popularization of few-shot prompting as given by Brown et al. (2020), several general approaches\n",
      "have improved the prompting ability of models, such as automatically learning prompts (Lester et al.,\n",
      "2021) or giving models instructions describing a task (Wei et al., 2022a; Sanh et al., 2022; Ouyang\n",
      "et al., 2022). Whereas these approaches improve or augment the input part of the prompt (e.g.,\n",
      "instructions that are prepended to inputs), our work takes the orthogonal direction of augmenting the\n",
      "outputs of language models with a chain of thought.\n",
      "8 Conclusions\n",
      "We have explored chain-of-thought prompting as a simple and broadly applicable method for enhanc-\n",
      "ing reasoning in language models. Through experiments on arithmetic, symbolic, and commonsense\n",
      "reasoning, we ﬁnd that chain-of-thought reasoning is an emergent property of model scale that allows\n",
      "sufﬁciently large language models to perform reasoning tasks that otherwise have ﬂat scaling curves.\n",
      "Broadening the range of reasoning tasks that language models can perform will hopefully inspire\n",
      "further work on language-based approaches to reasoning.\n",
      "9\n",
      "[4] C Extended Related Work\n",
      "Chain-of-thought prompting is a general approach that is inspired by several prior directions: prompt-\n",
      "ing, natural language explanations, program synthesis/execution, numeric and logical reasoning, and\n",
      "intermediate language steps.\n",
      "C.1 Prompting\n",
      "The recent success of large-scale language models has led to growing interest in improving their\n",
      "capability to perform tasks via prompting (Brown et al. (2020), and see Liu et al. (2021) for a\n",
      "survey). This paper falls in the category of general prompting approaches, whereby input prompts are\n",
      "optimized to allow a single large language model to better perform a variety of tasks (Li and Liang,\n",
      "2021; Lester et al., 2021; Reif et al., 2022, inter alia ).\n",
      "One recent line of work aims to improve the ability of language models to perform a task by providing\n",
      "instructions that describe the task (Raffel et al., 2020; Wei et al., 2022a; Ouyang et al., 2022; Sanh\n",
      "et al., 2022; Wang et al., 2022b). This line of work is related because it also augments input–output\n",
      "pairs with meta-data. But whereas an instruction augments the input to a task (instructions are typically\n",
      "prepended to the inputs), chain-of-thought prompting augments the outputs of language models.\n",
      "Another related direction is sequentially combining the outputs of language models; human–computer\n",
      "interaction (HCI) work (Wu et al., 2022a,b) has shown that combining sequential generations of\n",
      "language models improves task outcomes in a 20-person user study.\n",
      "C.2 Natural language explanations\n",
      "Another closely related direction uses natural language explanations (NLEs), often with the goal of\n",
      "improving model interpretability (Zhou et al., 2020; Wiegreffe and Marasovi ´c, 2021, inter alia ). That\n",
      "line of work typically focuses on natural language inference (Camburu et al., 2018; Yordanov et al.,\n",
      "2021; Bostrom et al., 2021), and produces explanations either simultaneously to or after the ﬁnal\n",
      "prediction (Narang et al., 2020; Majumder et al., 2021; Wiegreffe et al., 2021, 2022). By contrast,\n",
      "the chain of thought processing considered in this paper occurs before the ﬁnal answer. And while\n",
      "NLE aims mostly to improve neural network interpretability (Rajagopal et al., 2021), the goal of\n",
      "chain-of-thought prompting is to allow models to decompose multi-hop reasoning tasks into multiple\n",
      "steps—interpretability is just a side effect. Marasovi ´c et al. (2022) show that prompt-based ﬁnetuning\n",
      "with NLE improves NLI and classiﬁcation performance, though they largely focus on evaluating\n",
      "explanation plausibility. In comparison, our work focuses on a range of arithmetic, commonsense,\n",
      "and symbolic tasks that require multi-hop reasoning.\n",
      "C.3 Program synthesis and execution\n",
      "Using intermediate reasoning steps has a long history in program synthesis and execution (Zaremba\n",
      "and Sutskever, 2014, inter alia ). Recent work along in this direction has included a number of\n",
      "architectural innovations (Cai et al., 2017; Dong et al., 2019; Yan et al., 2020), as well as the use of\n",
      "large language models (Chen et al., 2021; Austin et al., 2021). The program execution work closest to\n",
      "ours is perhaps Nye et al. (2021), which show that large language models can perform up to 10-digit\n",
      "addition, evaluate polynomials, and execute python programs. Whereas generating a program and\n",
      "then executing it can be viewed as a type of reasoning, our work generalizes such domain-speciﬁc\n",
      "primitives to natural language, which is open-domain and relevant to any text-to-text NLP task in\n",
      "principle.\n",
      "C.4 Numeric and logical reasoning\n",
      "Numeric and logical reasoning has been a long-studied task in machine learning and natural language\n",
      "processing (Lev et al., 2004, inter alia ). Recent work has also aimed to inject numeric reasoning\n",
      "abilities in language models in various ways, such as augmenting BERT with a predeﬁned set of\n",
      "executable operations (Andor et al., 2019), including a graph neural network (Ran et al., 2019), and\n",
      "using specialized training procedures (Pi˛ ekos et al., 2021). Another line of work aims to enable\n",
      "language models to perform logical or formal reasoning, often by verablizing the rules in natural\n",
      "language formal rules using language (Clark et al., 2020; Saeed et al., 2021; Liang et al., 2021).\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "sample_query = \"chain-of-thought 개념을 알려줘\"\n",
    "print(f\"[Query]\\n{sample_query}\\n\")\n",
    "relevant_docs = bm25_retriever.get_relevant_documents(sample_query)\n",
    "print(\"[BM25 Retriever]\")\n",
    "pretty_print(relevant_docs)\n",
    "print(\"===\" * 20)\n",
    "relevant_docs = faiss_retriever.get_relevant_documents(sample_query)\n",
    "print(\"[FAISS Retriever]\")\n",
    "pretty_print(relevant_docs)\n",
    "print(\"===\" * 20)\n",
    "relevant_docs = ensemble_retriever.get_relevant_documents(sample_query)\n",
    "print(\"[Ensemble Retriever]\")\n",
    "pretty_print(relevant_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG 템플릿 실험\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF Path: data2/test_paper_2.pdf\n",
      "문서의 수: 43\n",
      "============================================================\n",
      "[HUMAN]\n",
      "What is chain-of-thought?\n",
      "\n",
      "[AI]\n",
      "Chain-of-thought is a method that allows models to decompose multi-step problems into intermediate steps, aiding in reasoning tasks. It can be used for tasks such as math word problems, commonsense reasoning, and symbolic manipulation. Language models can generate chains of thought if demonstrations of chain-of-thought reasoning are provided in the exemplars for few-shot prompting.\n"
     ]
    }
   ],
   "source": [
    "# 단계 1: 문서 로드(Load Documents)\n",
    "# 문서를 로드하고, 청크로 나누고, 인덱싱합니다.\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# PDF 파일 로드. 파일의 경로 입력\n",
    "file_path = \"data2/test_paper_2.pdf\"\n",
    "loader = PyPDFLoader(file_path=file_path)\n",
    "\n",
    "# 단계 2: 문서 분할(Split Documents)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "\n",
    "split_docs = loader.load_and_split(text_splitter=text_splitter)\n",
    "\n",
    "# 단계 3, 4: 임베딩 & 벡터스토어 생성(Create Vectorstore)\n",
    "# 벡터스토어를 생성합니다.\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# 단계 5: 리트리버 생성(Create Retriever)\n",
    "# 사용자의 질문(query) 에 부합하는 문서를 검색합니다.\n",
    "\n",
    "# 유사도 높은 K 개의 문서를 검색합니다.\n",
    "k = 3\n",
    "\n",
    "# (Sparse) bm25 retriever and (Dense) faiss retriever 를 초기화 합니다.\n",
    "bm25_retriever = BM25Retriever.from_documents(split_docs)\n",
    "bm25_retriever.k = k\n",
    "\n",
    "faiss_vectorstore = FAISS.from_documents(split_docs, OpenAIEmbeddings())\n",
    "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "# initialize the ensemble retriever\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5]\n",
    ")\n",
    "\n",
    "# 단계 6: 프롬프트 생성(Create Prompt)\n",
    "# 프롬프트를 생성합니다.\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# 단계 7: 언어모델 생성(Create LLM)\n",
    "# 모델(LLM) 을 생성합니다.\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    # 검색한 문서 결과를 하나의 문단으로 합쳐줍니다.\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# 단계 8: 체인 생성(Create Chain)\n",
    "rag_chain = (\n",
    "    {\"context\": ensemble_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "question = \"What is chain-of-thought?\"\n",
    "response = rag_chain.invoke(question)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"PDF Path: {file_path}\")\n",
    "print(f\"문서의 수: {len(docs)}\")\n",
    "print(\"===\" * 20)\n",
    "print(f\"[HUMAN]\\n{question}\\n\")\n",
    "print(f\"[AI]\\n{response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
