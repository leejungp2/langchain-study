{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12-02 Practice 2: PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### í™˜ê²½ì„¤ì •\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API KEY ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API í‚¤ë¥¼ í™˜ê²½ë³€ìˆ˜ë¡œ ê´€ë¦¬í•˜ê¸° ìœ„í•œ ì„¤ì • íŒŒì¼\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API í‚¤ ì •ë³´ ë¡œë“œ\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith ì¶”ì ì„ ì‹œì‘í•©ë‹ˆë‹¤.\n",
      "[í”„ë¡œì íŠ¸ëª…]\n",
      "CH12-RAG-practice\n"
     ]
    }
   ],
   "source": [
    "# LangSmith ì¶”ì ì„ ì„¤ì •í•©ë‹ˆë‹¤. https://smith.langchain.com\n",
    "!pip install -qU langchain-teddynote\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ì´ë¦„ì„ ì…ë ¥í•©ë‹ˆë‹¤.\n",
    "logging.langsmith(\"CH12-RAG-practice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ì¶”ê°€ëœ ì½”ë“œ\n",
    "#%pip install --upgrade \"pydantic>=2.7.4\" langchain langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 \n",
    "from langchain import hub\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma, FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ì½”ë“œ ë¹„êµ>\n",
    "- bs4.SoupStrainer(\"main\", attrs={\"id\": [\"main-content\"]}) â†’ <main> íƒœê·¸ ì¤‘ì—ì„œ id=\"main-content\"ë¥¼ ê°€ì§„ íŠ¹ì • ìš”ì†Œì˜ í…ìŠ¤íŠ¸ë§Œ ê°€ì ¸ì˜´.\n",
    "- í˜ì´ì§€ì˜ íŠ¹ì • ì˜ì—­ë§Œ í¬ë¡¤ë§í•  ë•Œ ìœ ìš©.\n",
    "- ì˜ˆë¥¼ ë“¤ì–´ ë„¤ì´ë²„ ë‰´ìŠ¤ë‚˜ ë¸”ë¡œê·¸ì˜ ë³¸ë¬¸ë§Œ ê°€ì ¸ì˜¬ ë•Œ ì í•©."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF ë¡œë“œ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¬¸ì„œì˜ ìˆ˜: 43\n",
      "\n",
      "[í˜ì´ì§€ë‚´ìš©]\n",
      "language models can generate chains of thought if demonstrations of chain-of-thought reasoning are\n",
      "provided in the exemplars for few-shot prompting.\n",
      "Figure 1 shows an example of a model producing a chain of thought to solve a math word problem\n",
      "that it would have otherwise gotten incorrect. The chain of thought in this case resembles a solution\n",
      "and can interpreted as one, but we still opt to call it a chain of thought to better capture the idea that it\n",
      "mimics a step-by-step thought process for ar\n",
      "\n",
      "[metadata]\n",
      "{'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-01-12T01:06:30+00:00', 'author': '', 'keywords': '', 'moddate': '2023-01-12T01:06:30+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data2/test_paper_2.pdf', 'total_pages': 43, 'page': 1, 'page_label': '2'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# PDF íŒŒì¼ ë¡œë“œ. íŒŒì¼ì˜ ê²½ë¡œ ì…ë ¥\n",
    "loader = PyPDFLoader(\"data2/test_paper_2.pdf\")\n",
    "\n",
    "# í˜ì´ì§€ ë³„ ë¬¸ì„œ ë¡œë“œ\n",
    "docs = loader.load()\n",
    "text = docs[2].page_content[:500]\n",
    "print(f\"ë¬¸ì„œì˜ ìˆ˜: {len(docs)}\")\n",
    "\n",
    "# 10ë²ˆì§¸ í˜ì´ì§€ì˜ ë‚´ìš© ì¶œë ¥\n",
    "print(f\"\\n[í˜ì´ì§€ë‚´ìš©]\\n{docs[2].page_content[:500]}\")\n",
    "print(f\"\\n[metadata]\\n{docs[1].metadata}\\n\") #ì „ì²´ í˜ì´ì§€ ìˆ˜, íŒŒì¼ ê²½ë¡œ ë“±ì˜ ì •ë³´ í¬í•¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ì½”ë“œ ì„¤ëª…>\n",
    "- docs = loader.load()ëŠ” PDF íŒŒì¼ì„ í˜ì´ì§€ ë‹¨ìœ„ë¡œ ë‚˜ëˆˆ Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ëŠ” ìƒíƒœ\n",
    "- textëŠ” ë¬¸ìì—´(str)ë§Œ ì¶”ì¶œí•˜ë¯€ë¡œ ì‚¬ìš©í•˜ë ¤ë©´ Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•˜ëŠ” ì‘ì—…ì´ í•„ìš”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `docs[10]`\n",
    "    - docs ë¦¬ìŠ¤íŠ¸ëŠ” ì—¬ëŸ¬ ê°œì˜ ë¬¸ì„œë¥¼ ì €ì¥í•˜ëŠ” ë¦¬ìŠ¤íŠ¸.\n",
    "    - docs[10]ì€ 0ë¶€í„° ì‹œì‘í•˜ëŠ” ì¸ë±ì‹± ê¸°ì¤€ìœ¼ë¡œ 11ë²ˆì§¸ ë¬¸ì„œë¥¼ ê°€ì ¸ì˜´.\n",
    "- `.page_content[:500]`\n",
    "    - docs[10].page_content â†’ ë¬¸ì„œì˜ ë‚´ìš© (í…ìŠ¤íŠ¸)\n",
    "    - [:500] â†’ ì²˜ìŒ 500ìê¹Œì§€ë§Œ ì¶œë ¥í•˜ì—¬ ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì˜ë¼ì„œ ë³´ì—¬ì¤Œ.\n",
    "- `.metadata`\n",
    "    - docs[10].metadata â†’ ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„°(ì •ë³´) ì¶œë ¥.\n",
    "    - ë³´í†µ ë¬¸ì„œì˜ ì œëª©, URL, ì‘ì„±ì¼, ì¶œì²˜ ë“± ì¶”ê°€ì ì¸ ì •ë³´ê°€ í¬í•¨ë¨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë¬¸ì„œ ë¶„í• (Split Documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) CharacterTextSplitter\n",
    "\n",
    "ì´ê²ƒì€ ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•ì…ë‹ˆë‹¤. ì´ ë°©ë²•ì€ `ë¬¸ìë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• `í•©ë‹ˆë‹¤(ê¸°ë³¸ê°’ì€ \"\\n\\n\") ê·¸ë¦¬ê³  ì²­í¬ì˜ ê¸¸ì´ë¥¼ ë¬¸ìì˜ ìˆ˜ë¡œ ì¸¡ì •í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=10,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['language models can generate chains of thought if demonstrations of chain-of-thought reasoning are\\nprovided in the exemplars for few-shot prompting.\\nFigure 1 shows an example of a model producing a chain of thought to solve a math word problem\\nthat it would have otherwise gotten incorrect. The chain of thought in this case resembles a solution\\nand can interpreted as one, but we still opt to call it a chain of thought to better capture the idea that it\\nmimics a step-by-step thought process for ar']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=100, chunk_overlap=10, separator=\"\\n\\n\"\n",
    ")\n",
    "text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ë‘ ê°œì˜ ì¤„ë°”ê¿ˆ(\\n\\n)ì„ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì„œë¥¼ ë‚˜ëˆ”\n",
    "- ì¦‰, ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•˜ë ¤ê³  í•  ë•Œ ìœ ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 101, which is longer than the specified 100\n",
      "Created a chunk of size 109, which is longer than the specified 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['language models can generate chains of thought if demonstrations of chain-of-thought reasoning are',\n",
       " 'provided in the exemplars for few-shot prompting.',\n",
       " 'Figure 1 shows an example of a model producing a chain of thought to solve a math word problem',\n",
       " 'that it would have otherwise gotten incorrect. The chain of thought in this case resembles a solution',\n",
       " 'and can interpreted as one, but we still opt to call it a chain of thought to better capture the idea that it',\n",
       " 'mimics a step-by-step thought process for ar']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=10, separator=\"\\n\")\n",
    "text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['language models can generate chains of thought if demonstrations of chain-of-thought reasoning',\n",
       " 'reasoning are\\nprovided in the exemplars for few-shot prompting.\\nFigure 1 shows an example of a model',\n",
       " 'of a model producing a chain of thought to solve a math word problem\\nthat it would have otherwise',\n",
       " 'otherwise gotten incorrect. The chain of thought in this case resembles a solution\\nand can',\n",
       " 'can interpreted as one, but we still opt to call it a chain of thought to better capture the idea',\n",
       " 'the idea that it\\nmimics a step-by-step thought process for ar']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=10, separator=\" \")\n",
    "text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ê³µë°±(ìŠ¤í˜ì´ìŠ¤ \" \")ì„ ê¸°ì¤€ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë‚˜ëˆ”\n",
    "- ë¬¸ì¥ì´ ì•„ë‹ˆë¼ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ì²­í¬ë¥¼ ë‚˜ëˆ„ë ¤ëŠ” ëª©ì ì¼ ë•Œ ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100, separator=\" \")\n",
    "# text íŒŒì¼ì„ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ì¤ë‹ˆë‹¤.\n",
    "text_splitter.split_text(text)\n",
    "\n",
    "# documentë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ì¤ë‹ˆë‹¤.\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "len(split_docs)\n",
    "#print(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-01-12T01:06:30+00:00', 'author': '', 'keywords': '', 'moddate': '2023-01-12T01:06:30+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data2/test_paper_2.pdf', 'total_pages': 43, 'page': 0, 'page_label': '1'}, page_content='Chain-of-Thought Prompting Elicits Reasoning\\nin Large Language Models\\nJason Wei Xuezhi Wang Dale Schuurmans Maarten Bosma\\nBrian Ichter Fei Xia Ed H. Chi Quoc V . Le Denny Zhou\\nGoogle Research, Brain Team\\n{jasonwei,dennyzhou}@google.com\\nAbstract\\nWe explore how generating a chain of thought â€”a series of intermediate reasoning\\nstepsâ€”signiï¬cantly improves the ability of large language models to perform\\ncomplex reasoning. In particular, we show how such reasoning abilities emerge\\nnaturally in sufï¬ciently large language models via a simple method called chain-of-\\nthought prompting , where a few chain of thought demonstrations are provided as\\nexemplars in prompting.\\nExperiments on three large language models show that chain-of-thought prompting\\nimproves performance on a range of arithmetic, commonsense, and symbolic\\nreasoning tasks. The empirical gains can be striking. For instance, prompting a\\nPaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art\\naccuracy on the')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) RecursiveTextSplitter\n",
    "- ì´ í…ìŠ¤íŠ¸ ë¶„í• ê¸°ëŠ” `ì¼ë°˜ í…ìŠ¤íŠ¸ì— ê¶Œì¥`ë˜ëŠ” í…ìŠ¤íŠ¸ ë¶„í• ê¸°ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain íŒ¨í‚¤ì§€ì—ì„œ RecursiveCharacterTextSplitter í´ë˜ìŠ¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "recursive_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # ì •ë§ ì‘ì€ ì²­í¬ í¬ê¸°ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=10,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language models can generate chains of thought if demonstrations of chain-of-thought reasoning\n",
      "reasoning are\n",
      "provided in the exemplars for few-shot prompting.\n",
      "Figure 1 shows an example of a model\n",
      "of a model producing a chain of thought to solve a math word problem\n",
      "that it would have otherwise\n",
      "otherwise gotten incorrect. The chain of thought in this case resembles a solution\n",
      "and can\n",
      "can interpreted as one, but we still opt to call it a chain of thought to better capture the idea\n",
      "the idea that it\n",
      "mimics a step-by-step thought process for ar\n",
      "============================================================\n",
      "language models can generate chains of thought if demonstrations of chain-of-thought reasoning are\n",
      "provided in the exemplars for few-shot prompting.\n",
      "Figure 1 shows an example of a model producing a chain of thought to solve a math word problem\n",
      "that it would have otherwise gotten incorrect. The chain of thought in this case resembles a\n",
      "a solution\n",
      "and can interpreted as one, but we still opt to call it a chain of thought to better capture the\n",
      "the idea that it\n",
      "mimics a step-by-step thought process for ar\n"
     ]
    }
   ],
   "source": [
    "character_text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=100, chunk_overlap=10, separator=\" \"\n",
    ")\n",
    "for sent in character_text_splitter.split_text(text):\n",
    "    print(sent)\n",
    "print(\"===\" * 20)\n",
    "recursive_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100, chunk_overlap=10\n",
    ")\n",
    "for sent in recursive_text_splitter.split_text(text):\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ë¹„êµ>\n",
    "\n",
    "`CharacterTextSplitter` â†’ ë‹¨ìˆœíˆ ë¬¸ì ê°œìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ”.\n",
    "- ì†ë„ê°€ ë¹ ë¦„\n",
    "- ë¬¸ì¥ ê²½ê³„ë¥¼ ë³´ì¥í•˜ì§€ ì•ŠìŒ\n",
    "- ê³µë°±(separator)ì„ ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆŒ ìˆ˜ë„ ìˆìŒ\n",
    "\n",
    "`RecursiveCharacterTextSplitter` â†’ ë¬¸ì¥ì„ ìµœëŒ€í•œ ìì—°ìŠ¤ëŸ½ê²Œ ìœ ì§€í•˜ë©´ì„œ ë‚˜ëˆ”.\n",
    "- ë¬¸ë‹¨ â†’ ë¬¸ì¥ â†’ ë‹¨ì–´ ìˆœì„œë¡œ ì‘ì€ ë‹¨ìœ„ë¡œ ë¶„í• \n",
    "- ë¬¸ì¥ ê²½ê³„ë¥¼ ìœ ì§€í•˜ë ¤ê³  í•¨\n",
    "- ë¬¸ì„œ ìš”ì•½, ê²€ìƒ‰ ì‹œìŠ¤í…œ ë“±ì—ì„œ ë” ì í•©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\n', '\\n', ' ', '']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recursive_text_splitter ì— ê¸°ë³¸ ì§€ì •ëœ separators ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "recursive_text_splitter._separators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Semantic Similarity\n",
    "- `ì˜ë¯¸ì  ìœ ì‚¬ì„±`ì„ ê¸°ì¤€ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "ì¶œì²˜: [Greg Kamradtâ€™s Notebook](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/5_Levels_Of_Text_Splitting.ipynb)\n",
    "\n",
    "ë†’ì€ ìˆ˜ì¤€(high level)ì—ì„œ ë¬¸ì¥ìœ¼ë¡œ ë¶„í• í•œ ë‹¤ìŒ 3ê°œ ë¬¸ì¥ìœ¼ë¡œ ê·¸ë£¹í™”í•œ ë‹¤ìŒ ì„ë² ë”© ê³µê°„ì—ì„œ ìœ ì‚¬í•œ ë¬¸ì¥ì„ ë³‘í•©í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# ìµœì‹  ë²„ì „ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.\n",
    "%pip install -U langchain langchain_experimental -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# SemanticChunker ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "semantic_text_splitter = SemanticChunker(OpenAIEmbeddings(), add_start_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language models can generate chains of thought if demonstrations of chain-of-thought reasoning are\n",
      "provided in the exemplars for few-shot prompting. Figure 1 shows an example of a model producing a chain of thought to solve a math word problem\n",
      "that it would have otherwise gotten incorrect.\n",
      "============================================================\n",
      "The chain of thought in this case resembles a solution\n",
      "and can interpreted as one, but we still opt to call it a chain of thought to better capture the idea that it\n",
      "mimics a step-by-step thought process for ar\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ë…¼ë¬¸ì˜ ì¼ë¶€ ë‚´ìš©ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤\n",
    "for sent in semantic_text_splitter.split_text(text):\n",
    "    print(sent)\n",
    "    print(\"===\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì„ë² ë”©\n",
    "= ì„ë² ë”©(Embedding)**ì´ë€ í…ìŠ¤íŠ¸(ë‹¨ì–´, ë¬¸ì¥, ë¬¸ì„œ)ë¥¼ ìˆ«ìë¡œ ë³€í™˜í•˜ëŠ” ê¸°ë²•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wd/rcmpfdjj6pb46wxhh3g97cvr0000gn/T/ipykernel_21743/1940278171.py:10: LangChainDeprecationWarning: Default values for HuggingFaceBgeEmbeddings.model_name were deprecated in LangChain 0.2.5 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceBgeEmbeddings constructor instead.\n",
      "  documents=splits, embedding=HuggingFaceBgeEmbeddings()\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=100, chunk_overlap=10, separator=\"\\n\\n\"\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents=splits, embedding=HuggingFaceBgeEmbeddings()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ì„¤ëª…>\n",
    "- í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥í•˜ê³  ê²€ìƒ‰í•  ìˆ˜ ìˆë„ë¡ ë²¡í„°ìŠ¤í† ì–´(Vectorstore)ë¥¼ ìƒì„±í•˜ëŠ” ê³¼ì •\n",
    "- Hugging Faceì˜ BGE ì„ë² ë”© ëª¨ë¸ì„ í™œìš©í•˜ì—¬ FAISS(Vector Database)ì— ì €ì¥í•˜ëŠ” ë°©ì‹ì´ ì‚¬ìš©ë¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#!pip uninstall fastembed -y\n",
    "%pip install fastembed -U -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=FastEmbedEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ì´ ì½”ë“œ ì›ë˜ ì—ëŸ¬ ë‚¬ì—ˆëŠ”ë° ì •ìƒ ì‘ë™\n",
    "- !pip -> %pip install ê´€ë ¨?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4ë‹¨ê³„: ë²¡í„°ìŠ¤í† ì–´ ìƒì„±(Create Vectorstore)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "= ë²¡í„° ìŠ¤í† ì–´(Vector Store)ëŠ” í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë²¡í„°(ìˆ«ì)ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥í•˜ê³ , ìœ ì‚¬í•œ ë²¡í„°ë¥¼ ë¹ ë¥´ê²Œ ê²€ìƒ‰í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ë°ì´í„°ë² ì´ìŠ¤\n",
    "\n",
    "ì¦‰, í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ì ìœ¼ë¡œ ê²€ìƒ‰í•  ìˆ˜ ìˆë„ë¡ ë³€í™˜í•˜ëŠ” ê³¼ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# FAISS DB ì ìš©\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Chroma DB ì ìš©\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retriever ìƒì„±"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰\n",
    "\n",
    "- ê¸°ë³¸ê°’ì€ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ì¸ `similarity` ê°€ ì ìš©ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'author': '', 'creationdate': '2023-01-12T01:06:30+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2023-01-12T01:06:30+00:00', 'page': 2, 'page_label': '3', 'producer': 'pdfTeX-1.40.21', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': 'data2/test_paper_2.pdf', 'subject': '', 'title': '', 'total_pages': 43, 'trapped': '/False'}, page_content='language models can generate chains of thought if demonstrations of chain-of-thought reasoning are\\nprovided in the exemplars for few-shot prompting.\\nFigure 1 shows an example of a model producing a chain of thought to solve a math word problem\\nthat it would have otherwise gotten incorrect. The chain of thought in this case resembles a solution\\nand can interpreted as one, but we still opt to call it a chain of thought to better capture the idea that it\\nmimics a step-by-step thought process for arriving at the answer (and also, solutions/explanations\\ntypically come after the ï¬nal answer (Narang et al., 2020; Wiegreffe et al., 2022; Lampinen et al.,\\n2022, inter alia )).\\nChain-of-thought prompting has several attractive properties as an approach for facilitating reasoning\\nin language models.\\n1.First, chain of thought, in principle, allows models to decompose multi-step problems into\\nintermediate steps, which means that additional computation can be allocated to problems\\nthat require more reasoning steps.\\n2.Second, a chain of thought provides an interpretable window into the behavior of the model,\\nsuggesting how it might have arrived at a particular answer and providing opportunities\\nto debug where the reasoning path went wrong (although fully characterizing a modelâ€™s\\ncomputations that support an answer remains an open question).\\n3.Third, chain-of-thought reasoning can be used for tasks such as math word problems,\\ncommonsense reasoning, and symbolic manipulation, and is potentially applicable (at least\\nin principle) to any task that humans can solve via language.\\n4.Finally, chain-of-thought reasoning can be readily elicited in sufï¬ciently large off-the-shelf\\nlanguage models simply by including examples of chain of thought sequences into the\\nexemplars of few-shot prompting.\\nIn empirical experiments, we will observe the utility of chain-of-thought prompting for arithmetic\\nreasoning (Section 3), commonsense reasoning (Section 4), and symbolic reasoning (Section 5).\\n3 Arithmetic Reasoning\\nWe begin by considering math word problems of the form in Figure 1, which measure the arithmetic\\nreasoning ability of language models. Though simple for humans, arithmetic reasoning is a task where\\nlanguage models often struggle (Hendrycks et al., 2021; Patel et al., 2021, inter alia ). Strikingly, chain-\\nof-thought prompting when used with the 540B parameter language model performs comparably with\\ntask-speciï¬c ï¬netuned models on several tasks, even achieving new state of the art on the challenging\\nGSM8K benchmark (Cobbe et al., 2021).\\n3.1 Experimental Setup\\nWe explore chain-of-thought prompting for various language models on multiple benchmarks.\\nBenchmarks. We consider the following ï¬ve math word problem benchmarks: (1)theGSM8K\\nbenchmark of math word problems (Cobbe et al., 2021), (2)theSV AMP dataset of math word\\nproblems with varying structures (Patel et al., 2021), (3)theASDiv dataset of diverse math word\\nproblems (Miao et al., 2020), (4)theAQuA dataset of algebraic word problems, and (5)theMA WPS\\nbenchmark (Koncel-Kedziorski et al., 2016). Example problems are given in Appendix Table 12.\\nStandard prompting. For the baseline, we consider standard few-shot prompting, popularized by\\nBrown et al. (2020), in which a language model is given in-context exemplars of inputâ€“output pairs\\nbefore outputting a prediction for a test-time example. Exemplars are formatted as questions and\\nanswers. The model gives the answer directly, as shown in Figure 1 (left).\\nChain-of-thought prompting. Our proposed approach is to augment each exemplar in few-shot\\nprompting with a chain of thought for an associated answer, as illustrated in Figure 1 (right). As most\\nof the datasets only have an evaluation split, we manually composed a set of eight few-shot exemplars\\nwith chains of thought for promptingâ€”Figure 1 (right) shows one chain of thought exemplar, and the\\nfull set of exemplars is given in Appendix Table 20. (These particular exemplars did not undergo\\nprompt engineering; robustness is studied in Section 3.4 and Appendix A.2.) To investigate whether\\nchain-of-thought prompting in this form can successfully elicit successful reasoning across a range of\\n3'), Document(metadata={'author': '', 'creationdate': '2023-01-12T01:06:30+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2023-01-12T01:06:30+00:00', 'page': 2, 'page_label': '3', 'producer': 'pdfTeX-1.40.21', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': 'data2/test_paper_2.pdf', 'subject': '', 'title': '', 'total_pages': 43, 'trapped': '/False'}, page_content='language models can generate chains of thought if demonstrations of chain-of-thought reasoning are\\nprovided in the exemplars for few-shot prompting.\\nFigure 1 shows an example of a model producing a chain of thought to solve a math word problem\\nthat it would have otherwise gotten incorrect. The chain of thought in this case resembles a solution\\nand can interpreted as one, but we still opt to call it a chain of thought to better capture the idea that it\\nmimics a step-by-step thought process for arriving at the answer (and also, solutions/explanations\\ntypically come after the ï¬nal answer (Narang et al., 2020; Wiegreffe et al., 2022; Lampinen et al.,\\n2022, inter alia )).\\nChain-of-thought prompting has several attractive properties as an approach for facilitating reasoning\\nin language models.\\n1.First, chain of thought, in principle, allows models to decompose multi-step problems into\\nintermediate steps, which means that additional computation can be allocated to problems\\nthat require more reasoning steps.\\n2.Second, a chain of thought provides an interpretable window into the behavior of the model,\\nsuggesting how it might have arrived at a particular answer and providing opportunities\\nto debug where the reasoning path went wrong (although fully characterizing a modelâ€™s\\ncomputations that support an answer remains an open question).\\n3.Third, chain-of-thought reasoning can be used for tasks such as math word problems,\\ncommonsense reasoning, and symbolic manipulation, and is potentially applicable (at least\\nin principle) to any task that humans can solve via language.\\n4.Finally, chain-of-thought reasoning can be readily elicited in sufï¬ciently large off-the-shelf\\nlanguage models simply by including examples of chain of thought sequences into the\\nexemplars of few-shot prompting.\\nIn empirical experiments, we will observe the utility of chain-of-thought prompting for arithmetic\\nreasoning (Section 3), commonsense reasoning (Section 4), and symbolic reasoning (Section 5).\\n3 Arithmetic Reasoning\\nWe begin by considering math word problems of the form in Figure 1, which measure the arithmetic\\nreasoning ability of language models. Though simple for humans, arithmetic reasoning is a task where\\nlanguage models often struggle (Hendrycks et al., 2021; Patel et al., 2021, inter alia ). Strikingly, chain-\\nof-thought prompting when used with the 540B parameter language model performs comparably with\\ntask-speciï¬c ï¬netuned models on several tasks, even achieving new state of the art on the challenging\\nGSM8K benchmark (Cobbe et al., 2021).\\n3.1 Experimental Setup\\nWe explore chain-of-thought prompting for various language models on multiple benchmarks.\\nBenchmarks. We consider the following ï¬ve math word problem benchmarks: (1)theGSM8K\\nbenchmark of math word problems (Cobbe et al., 2021), (2)theSV AMP dataset of math word\\nproblems with varying structures (Patel et al., 2021), (3)theASDiv dataset of diverse math word\\nproblems (Miao et al., 2020), (4)theAQuA dataset of algebraic word problems, and (5)theMA WPS\\nbenchmark (Koncel-Kedziorski et al., 2016). Example problems are given in Appendix Table 12.\\nStandard prompting. For the baseline, we consider standard few-shot prompting, popularized by\\nBrown et al. (2020), in which a language model is given in-context exemplars of inputâ€“output pairs\\nbefore outputting a prediction for a test-time example. Exemplars are formatted as questions and\\nanswers. The model gives the answer directly, as shown in Figure 1 (left).\\nChain-of-thought prompting. Our proposed approach is to augment each exemplar in few-shot\\nprompting with a chain of thought for an associated answer, as illustrated in Figure 1 (right). As most\\nof the datasets only have an evaluation split, we manually composed a set of eight few-shot exemplars\\nwith chains of thought for promptingâ€”Figure 1 (right) shows one chain of thought exemplar, and the\\nfull set of exemplars is given in Appendix Table 20. (These particular exemplars did not undergo\\nprompt engineering; robustness is studied in Section 3.4 and Appendix A.2.) To investigate whether\\nchain-of-thought prompting in this form can successfully elicit successful reasoning across a range of\\n3'), Document(metadata={'author': '', 'creationdate': '2023-01-12T01:06:30+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2023-01-12T01:06:30+00:00', 'page': 8, 'page_label': '9', 'producer': 'pdfTeX-1.40.21', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': 'data2/test_paper_2.pdf', 'subject': '', 'title': '', 'total_pages': 43, 'trapped': '/False'}, page_content='experiments on commonsense reasoning underscored how the linguistic nature of chain-of-thought\\nreasoning makes it generally applicable (Section 4). Finally, we showed that for symbolic reasoning,\\nchain-of-thought prompting facilitates OOD generalization to longer sequence lengths (Section 5). In\\nall experiments, chain-of-thought reasoning is elicited simply by prompting an off-the-shelf language\\nmodel. No language models were ï¬netuned in the process of writing this paper.\\nThe emergence of chain-of-thought reasoning as a result of model scale has been a prevailing theme\\n(Wei et al., 2022b). For many reasoning tasks where standard prompting has a ï¬‚at scaling curve, chain-\\nof-thought prompting leads to dramatically increasing scaling curves. Chain-of-thought prompting\\nappears to expand the set of tasks that large language models can perform successfullyâ€”in other\\nwords, our work underscores that standard prompting only provides a lower bound on the capabilities\\nof large language models. This observation likely raises more questions than it answersâ€”for instance,\\nhow much more can we expect reasoning ability to improve with a further increase in model scale?\\nWhat other prompting methods might expand the range of tasks that language models can solve?\\nAs for limitations, we ï¬rst qualify that although chain of thought emulates the thought processes of\\nhuman reasoners, this does not answer whether the neural network is actually â€œreasoning,â€ which\\nwe leave as an open question. Second, although the cost of manually augmenting exemplars with\\nchains of thought is minimal in the few-shot setting, such annotation costs could be prohibitive for\\nï¬netuning (though this could potentially be surmounted with synthetic data generation, or zero-shot\\ngeneralization). Third, there is no guarantee of correct reasoning paths, which can lead to both correct\\nand incorrect answers; improving factual generations of language models is an open direction for\\nfuture work (Rashkin et al., 2021; Ye and Durrett, 2022; Wiegreffe et al., 2022, inter alia ). Finally,\\nthe emergence of chain-of-thought reasoning only at large model scales makes it costly to serve in\\nreal-world applications; further research could explore how to induce reasoning in smaller models.\\n7 Related Work\\nThis work is inspired by many research areas, which we detail in an extended related work section\\n(Appendix C). Here we describe two directions and associated papers that are perhaps most relevant.\\nThe ï¬rst relevant direction is using intermediate steps to solve reasoning problems. Ling et al. (2017)\\npioneer the idea of using natural language rationales to solve math word problems through a series\\nof intermediate steps. Their work is a remarkable contrast to the literature using formal languages\\nto reason (Roy et al., 2015; Chiang and Chen, 2019; Amini et al., 2019; Chen et al., 2019). Cobbe\\net al. (2021) extend Ling et al. (2017) by creating a larger dataset and using it to ï¬netune a pretrained\\nlanguage model rather than training a model from scratch. In the domain of program synthesis,\\nNye et al. (2021) leverage language models to predict the ï¬nal outputs of Python programs via\\nï¬rst line-to-line predicting the intermediate computational results, and show that their step-by-step\\nprediction method performs better than directly predicting the ï¬nal outputs.\\nNaturally, this paper also relates closely to the large body of recent work on prompting. Since the\\npopularization of few-shot prompting as given by Brown et al. (2020), several general approaches\\nhave improved the prompting ability of models, such as automatically learning prompts (Lester et al.,\\n2021) or giving models instructions describing a task (Wei et al., 2022a; Sanh et al., 2022; Ouyang\\net al., 2022). Whereas these approaches improve or augment the input part of the prompt (e.g.,\\ninstructions that are prepended to inputs), our work takes the orthogonal direction of augmenting the\\noutputs of language models with a chain of thought.\\n8 Conclusions\\nWe have explored chain-of-thought prompting as a simple and broadly applicable method for enhanc-\\ning reasoning in language models. Through experiments on arithmetic, symbolic, and commonsense\\nreasoning, we ï¬nd that chain-of-thought reasoning is an emergent property of model scale that allows\\nsufï¬ciently large language models to perform reasoning tasks that otherwise have ï¬‚at scaling curves.\\nBroadening the range of reasoning tasks that language models can perform will hopefully inspire\\nfurther work on language-based approaches to reasoning.\\n9'), Document(metadata={'author': '', 'creationdate': '2023-01-12T01:06:30+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2023-01-12T01:06:30+00:00', 'page': 8, 'page_label': '9', 'producer': 'pdfTeX-1.40.21', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': 'data2/test_paper_2.pdf', 'subject': '', 'title': '', 'total_pages': 43, 'trapped': '/False'}, page_content='experiments on commonsense reasoning underscored how the linguistic nature of chain-of-thought\\nreasoning makes it generally applicable (Section 4). Finally, we showed that for symbolic reasoning,\\nchain-of-thought prompting facilitates OOD generalization to longer sequence lengths (Section 5). In\\nall experiments, chain-of-thought reasoning is elicited simply by prompting an off-the-shelf language\\nmodel. No language models were ï¬netuned in the process of writing this paper.\\nThe emergence of chain-of-thought reasoning as a result of model scale has been a prevailing theme\\n(Wei et al., 2022b). For many reasoning tasks where standard prompting has a ï¬‚at scaling curve, chain-\\nof-thought prompting leads to dramatically increasing scaling curves. Chain-of-thought prompting\\nappears to expand the set of tasks that large language models can perform successfullyâ€”in other\\nwords, our work underscores that standard prompting only provides a lower bound on the capabilities\\nof large language models. This observation likely raises more questions than it answersâ€”for instance,\\nhow much more can we expect reasoning ability to improve with a further increase in model scale?\\nWhat other prompting methods might expand the range of tasks that language models can solve?\\nAs for limitations, we ï¬rst qualify that although chain of thought emulates the thought processes of\\nhuman reasoners, this does not answer whether the neural network is actually â€œreasoning,â€ which\\nwe leave as an open question. Second, although the cost of manually augmenting exemplars with\\nchains of thought is minimal in the few-shot setting, such annotation costs could be prohibitive for\\nï¬netuning (though this could potentially be surmounted with synthetic data generation, or zero-shot\\ngeneralization). Third, there is no guarantee of correct reasoning paths, which can lead to both correct\\nand incorrect answers; improving factual generations of language models is an open direction for\\nfuture work (Rashkin et al., 2021; Ye and Durrett, 2022; Wiegreffe et al., 2022, inter alia ). Finally,\\nthe emergence of chain-of-thought reasoning only at large model scales makes it costly to serve in\\nreal-world applications; further research could explore how to induce reasoning in smaller models.\\n7 Related Work\\nThis work is inspired by many research areas, which we detail in an extended related work section\\n(Appendix C). Here we describe two directions and associated papers that are perhaps most relevant.\\nThe ï¬rst relevant direction is using intermediate steps to solve reasoning problems. Ling et al. (2017)\\npioneer the idea of using natural language rationales to solve math word problems through a series\\nof intermediate steps. Their work is a remarkable contrast to the literature using formal languages\\nto reason (Roy et al., 2015; Chiang and Chen, 2019; Amini et al., 2019; Chen et al., 2019). Cobbe\\net al. (2021) extend Ling et al. (2017) by creating a larger dataset and using it to ï¬netune a pretrained\\nlanguage model rather than training a model from scratch. In the domain of program synthesis,\\nNye et al. (2021) leverage language models to predict the ï¬nal outputs of Python programs via\\nï¬rst line-to-line predicting the intermediate computational results, and show that their step-by-step\\nprediction method performs better than directly predicting the ï¬nal outputs.\\nNaturally, this paper also relates closely to the large body of recent work on prompting. Since the\\npopularization of few-shot prompting as given by Brown et al. (2020), several general approaches\\nhave improved the prompting ability of models, such as automatically learning prompts (Lester et al.,\\n2021) or giving models instructions describing a task (Wei et al., 2022a; Sanh et al., 2022; Ouyang\\net al., 2022). Whereas these approaches improve or augment the input part of the prompt (e.g.,\\ninstructions that are prepended to inputs), our work takes the orthogonal direction of augmenting the\\noutputs of language models with a chain of thought.\\n8 Conclusions\\nWe have explored chain-of-thought prompting as a simple and broadly applicable method for enhanc-\\ning reasoning in language models. Through experiments on arithmetic, symbolic, and commonsense\\nreasoning, we ï¬nd that chain-of-thought reasoning is an emergent property of model scale that allows\\nsufï¬ciently large language models to perform reasoning tasks that otherwise have ï¬‚at scaling curves.\\nBroadening the range of reasoning tasks that language models can perform will hopefully inspire\\nfurther work on language-based approaches to reasoning.\\n9')]\n"
     ]
    }
   ],
   "source": [
    "query = \"What does 'chain of thought' mean?\"\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\")\n",
    "search_result = retriever.get_relevant_documents(query)\n",
    "print(search_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`similarity_score_threshold` ëŠ” ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰ì—ì„œ `score_threshold` ì´ìƒì¸ ê²°ê³¼ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Œ [ë¬¸ì„œ ë©”íƒ€ë°ì´í„°]: {'author': '', 'creationdate': '2023-01-12T01:06:30+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2023-01-12T01:06:30+00:00', 'page': 8, 'page_label': '9', 'producer': 'pdfTeX-1.40.21', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': 'data2/test_paper_2.pdf', 'subject': '', 'title': '', 'total_pages': 43, 'trapped': '/False'}\n",
      "ğŸ“„ [ë¬¸ì„œ ë‚´ìš©]: experiments on commonsense reasoning underscored how the linguistic nature of chain-of-thought\n",
      "reasoning makes it generally applicable (Section 4). Finally, we showed that for symbolic reasoning,\n",
      "chain-of-thought prompting facilitates OOD generalization to longer sequence lengths (Section 5). In\n",
      "all experiments, chain-of-thought reasoning is elicited simply by prompting an off-the-shelf language\n",
      "model. No language models were ï¬netuned in the process of writing this paper.\n",
      "The emergence of chain-\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "query = \"What does 'chain-of-thought' mean?\"\n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",  # `k` ê°’ì„ ì ìš©í•  ìˆ˜ ìˆë„ë¡ ì„¤ì •\n",
    "    search_kwargs={\"k\": 1}  # ìµœëŒ€ 1ê°œì˜ ë¬¸ì„œë§Œ ë°˜í™˜í•˜ë„ë¡ ì œí•œ\n",
    ")\n",
    "\n",
    "search_result = retriever.get_relevant_documents(query)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥ (ìµœëŒ€ 1ê°œë§Œ ì¶œë ¥)\n",
    "if not search_result:\n",
    "    print(\"âŒ No relevant documents found.\")\n",
    "else:\n",
    "    doc = search_result[0]  # ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ 1ê°œë§Œ ê°€ì ¸ì˜´\n",
    "    print(f\"\\nğŸ“Œ [ë¬¸ì„œ ë©”íƒ€ë°ì´í„°]: {doc.metadata}\")\n",
    "    print(f\"ğŸ“„ [ë¬¸ì„œ ë‚´ìš©]: {doc.page_content[:500]}\")  # 500ìê¹Œì§€ë§Œ ì¶œë ¥\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No relevant docs were retrieved using the relevance score threshold 0.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "âŒ No relevant documents found.\n"
     ]
    }
   ],
   "source": [
    "query = \"What does 'chain-of-thought' mean?\"\n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.8, \"k\": 1}\n",
    ")\n",
    "search_result = retriever.get_relevant_documents(query)\n",
    "print(search_result)\n",
    "if not search_result:\n",
    "    print(\"âŒ No relevant documents found.\")\n",
    "else:\n",
    "    doc = search_result[0]  # ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ 1ê°œë§Œ ê°€ì ¸ì˜´\n",
    "    print(f\"\\nğŸ“Œ [ë¬¸ì„œ ë©”íƒ€ë°ì´í„°]: {doc.metadata}\")\n",
    "    print(f\"ğŸ“„ [ë¬¸ì„œ ë‚´ìš©]: {doc.page_content}\")  # 500ìê¹Œì§€ë§Œ ì¶œë ¥\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`maximum marginal search result` ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Œ [ë¬¸ì„œ ë©”íƒ€ë°ì´í„°]: {'author': '', 'creationdate': '2023-01-12T01:06:30+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2023-01-12T01:06:30+00:00', 'page': 8, 'page_label': '9', 'producer': 'pdfTeX-1.40.21', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': 'data2/test_paper_2.pdf', 'subject': '', 'title': '', 'total_pages': 43, 'trapped': '/False'}\n",
      "ğŸ“„ [ë¬¸ì„œ ë‚´ìš©]: experiments on commonsense reasoning underscored how the linguistic nature of chain-of-thought\n",
      "reasoning makes it generally applicable (Section 4). Finally, we showed that for symbolic reasoning,\n",
      "chain-of-thought prompting facilitates OOD generalization to longer sequence lengths (Section 5). In\n",
      "all experiments, chain-of-thought reasoning is elicited simply by prompting an off-the-shelf language\n",
      "model. No language models were ï¬netuned in the process of writing this paper.\n",
      "The emergence of chain-\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "query = \"What does 'chain-of-thought' mean?\"\n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",  # `k` ê°’ì„ ì ìš©í•  ìˆ˜ ìˆë„ë¡ ì„¤ì •\n",
    "    search_kwargs={\"k\": 2}  # ìµœëŒ€ 2ê°œì˜ ë¬¸ì„œë§Œ ë°˜í™˜í•˜ë„ë¡ ì œí•œ\n",
    ")\n",
    "\n",
    "search_result = retriever.get_relevant_documents(query)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥ (ìµœëŒ€ 1ê°œë§Œ ì¶œë ¥)\n",
    "if not search_result:\n",
    "    print(\"âŒ No relevant documents found.\")\n",
    "else:\n",
    "    doc = search_result[0]  # ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ 1ê°œë§Œ ê°€ì ¸ì˜´\n",
    "    print(f\"\\nğŸ“Œ [ë¬¸ì„œ ë©”íƒ€ë°ì´í„°]: {doc.metadata}\")\n",
    "    print(f\"ğŸ“„ [ë¬¸ì„œ ë‚´ìš©]: {doc.page_content[:500]}\")  # 500ìê¹Œì§€ë§Œ ì¶œë ¥\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë‹¤ì–‘í•œ ì¿¼ë¦¬ ìƒì„±\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "query = \"\"\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectorstore.as_retriever(), llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logging for the queries\n",
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. Can you explain the concept of chain-of-thought prompting?', '2. How does chain-of-thought prompting work?', '3. What are the key aspects of chain-of-thought prompting?']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is chain-of-thought prompting?\"\n",
    "unique_docs = retriever_from_llm.get_relevant_documents(query=question)\n",
    "len(unique_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Retrievers initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# âœ… `page_content`ë§Œ ì¶”ì¶œí•´ì„œ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜*\n",
    "doc_texts = [d.page_content for d in docs]  # `BM25Retriever`ì™€ `FAISS`ì—ì„œ ì‚¬ìš© ê°€ëŠ¥\n",
    "\n",
    "# âœ… BM25 Retriever ì´ˆê¸°í™”\n",
    "bm25_retriever = BM25Retriever.from_texts(doc_texts)  \n",
    "bm25_retriever.k = 2  # ê²€ìƒ‰í•  ìµœëŒ€ ë¬¸ì„œ ê°œìˆ˜\n",
    "\n",
    "# âœ… FAISS Retriever ì´ˆê¸°í™”\n",
    "faiss_vectorstore = FAISS.from_texts(doc_texts, OpenAIEmbeddings())\n",
    "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# âœ… Ensemble Retriever ì´ˆê¸°í™” (BM25 + FAISS)\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, faiss_retriever], \n",
    "    weights=[0.5, 0.5]  # BM25ì™€ FAISSì— ë™ì¼í•œ ê°€ì¤‘ì¹˜ ì ìš©\n",
    ")\n",
    "\n",
    "print(\"âœ… Retrievers initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print(docs):\n",
    "    for i, doc in enumerate(docs):\n",
    "        print(f\"[{i+1}] {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Query]\n",
      "chain-of-thought ê°œë…ì„ ì•Œë ¤ì¤˜\n",
      "\n",
      "[BM25 Retriever]\n",
      "[1] 0204060GSM8K\n",
      "solve rate (%)LaMDA GPT PaLMStandard prompting\n",
      "Chain-of-thought prompting\n",
      "Prior supervised best\n",
      "020406080SV AMP\n",
      "solve rate (%)\n",
      "0.4 81370255075100MAWPS\n",
      "solve rate (%)\n",
      "0.4 7175 862540\n",
      "Model scale (# parameters in billions)\n",
      "Figure 4: Chain-of-thought prompting enables\n",
      "large language models to solve challenging math\n",
      "problems. Notably, chain-of-thought reasoning\n",
      "is an emergent ability of increasing model scale.\n",
      "Prior best numbers are from Cobbe et al. (2021)\n",
      "for GSM8K, Jie et al. (2022) for SV AMP, and Lan\n",
      "et al. (2021) for MAWPS.Second, chain-of-thought prompting has larger\n",
      "performance gains for more-complicated prob-\n",
      "lems. For instance, for GSM8K (the dataset\n",
      "with the lowest baseline performance), perfor-\n",
      "mance more than doubled for the largest GPT\n",
      "and PaLM models. On the other hand, for Sin-\n",
      "gleOp, the easiest subset of MAWPS which only\n",
      "requires a single step to solve, performance im-\n",
      "provements were either negative or very small\n",
      "(see Appendix Table 3).\n",
      "Third, chain-of-thought prompting via GPT-3\n",
      "175B and PaLM 540B compares favorably to\n",
      "prior state of the art, which typically ï¬netunes a\n",
      "task-speciï¬c model on a labeled training dataset.\n",
      "Figure 4 shows how PaLM 540B uses chain-of-\n",
      "thought prompting to achieve new state of the art\n",
      "on GSM8K, SV AMP, and MAWPS (though note\n",
      "that standard prompting already passed the prior\n",
      "best for SV AMP). On the other two datasets,\n",
      "AQuA and ASDiv, PaLM with chain-of-thought\n",
      "prompting reaches within 2% of the state of the\n",
      "art (Appendix Table 2).\n",
      "To better understand why chain-of-thought\n",
      "prompting works, we manually examined model-\n",
      "generated chains of thought by LaMDA 137B\n",
      "for GSM8K. Of 50 random examples where the\n",
      "model returned the correct ï¬nal answer, all of\n",
      "the generated chains of thought were also log-\n",
      "ically and mathematically correct except two\n",
      "that coincidentally arrived at the correct answer\n",
      "(see Appendix D.1, and Table 8 for examples\n",
      "of correct model-generated chains of thought).\n",
      "We also randomly examined 50 random sam-\n",
      "ples for which the model gave the wrong answer.\n",
      "The summary of this analysis is that 46% of the\n",
      "chains of thought were almost correct, barring\n",
      "minor mistakes (calculator error, symbol map-\n",
      "ping error, or one reasoning step missing), and that the other 54% of the chains of thought had major\n",
      "errors in semantic understanding or coherence (see Appendix D.2). To provide a small insight into\n",
      "why scaling improves chain-of-thought reasoning ability, we performed a similar analysis of errors\n",
      "made by PaLM 62B and whether those errors were ï¬xed by scaling to PaLM 540B. The summary\n",
      "is that scaling PaLM to 540B ï¬xes a large portion of one-step missing and semantic understanding\n",
      "errors in the 62B model (see Appendix A.1).\n",
      "3.3 Ablation Study\n",
      "The observed beneï¬ts of using chain-of-thought prompting raises the natural question of whether the\n",
      "same performance improvements can be conferred via other types of prompting. Figure 5 shows an\n",
      "ablation study with three variations of chain of thought described below.\n",
      "Equation only. One reason for why chain-of-thought prompting might help is that it produces the\n",
      "mathematical equation to be evaluated, and so we test a variation where the model is prompted\n",
      "to output only a mathematical equation before giving the answer. Figure 5 shows that equation\n",
      "only prompting does not help much for GSM8K, which implies that the semantics of the questions\n",
      "in GSM8K are too challenging to directly translate into an equation without the natural language\n",
      "reasoning steps in chain of thought. For datasets of one-step or two-step problems, however, we ï¬nd\n",
      "that equation only prompting does improve performance, since the equation can be easily derived\n",
      "from the question (see Appendix Table 6).\n",
      "5\n",
      "[2] experiments on commonsense reasoning underscored how the linguistic nature of chain-of-thought\n",
      "reasoning makes it generally applicable (Section 4). Finally, we showed that for symbolic reasoning,\n",
      "chain-of-thought prompting facilitates OOD generalization to longer sequence lengths (Section 5). In\n",
      "all experiments, chain-of-thought reasoning is elicited simply by prompting an off-the-shelf language\n",
      "model. No language models were ï¬netuned in the process of writing this paper.\n",
      "The emergence of chain-of-thought reasoning as a result of model scale has been a prevailing theme\n",
      "(Wei et al., 2022b). For many reasoning tasks where standard prompting has a ï¬‚at scaling curve, chain-\n",
      "of-thought prompting leads to dramatically increasing scaling curves. Chain-of-thought prompting\n",
      "appears to expand the set of tasks that large language models can perform successfullyâ€”in other\n",
      "words, our work underscores that standard prompting only provides a lower bound on the capabilities\n",
      "of large language models. This observation likely raises more questions than it answersâ€”for instance,\n",
      "how much more can we expect reasoning ability to improve with a further increase in model scale?\n",
      "What other prompting methods might expand the range of tasks that language models can solve?\n",
      "As for limitations, we ï¬rst qualify that although chain of thought emulates the thought processes of\n",
      "human reasoners, this does not answer whether the neural network is actually â€œreasoning,â€ which\n",
      "we leave as an open question. Second, although the cost of manually augmenting exemplars with\n",
      "chains of thought is minimal in the few-shot setting, such annotation costs could be prohibitive for\n",
      "ï¬netuning (though this could potentially be surmounted with synthetic data generation, or zero-shot\n",
      "generalization). Third, there is no guarantee of correct reasoning paths, which can lead to both correct\n",
      "and incorrect answers; improving factual generations of language models is an open direction for\n",
      "future work (Rashkin et al., 2021; Ye and Durrett, 2022; Wiegreffe et al., 2022, inter alia ). Finally,\n",
      "the emergence of chain-of-thought reasoning only at large model scales makes it costly to serve in\n",
      "real-world applications; further research could explore how to induce reasoning in smaller models.\n",
      "7 Related Work\n",
      "This work is inspired by many research areas, which we detail in an extended related work section\n",
      "(Appendix C). Here we describe two directions and associated papers that are perhaps most relevant.\n",
      "The ï¬rst relevant direction is using intermediate steps to solve reasoning problems. Ling et al. (2017)\n",
      "pioneer the idea of using natural language rationales to solve math word problems through a series\n",
      "of intermediate steps. Their work is a remarkable contrast to the literature using formal languages\n",
      "to reason (Roy et al., 2015; Chiang and Chen, 2019; Amini et al., 2019; Chen et al., 2019). Cobbe\n",
      "et al. (2021) extend Ling et al. (2017) by creating a larger dataset and using it to ï¬netune a pretrained\n",
      "language model rather than training a model from scratch. In the domain of program synthesis,\n",
      "Nye et al. (2021) leverage language models to predict the ï¬nal outputs of Python programs via\n",
      "ï¬rst line-to-line predicting the intermediate computational results, and show that their step-by-step\n",
      "prediction method performs better than directly predicting the ï¬nal outputs.\n",
      "Naturally, this paper also relates closely to the large body of recent work on prompting. Since the\n",
      "popularization of few-shot prompting as given by Brown et al. (2020), several general approaches\n",
      "have improved the prompting ability of models, such as automatically learning prompts (Lester et al.,\n",
      "2021) or giving models instructions describing a task (Wei et al., 2022a; Sanh et al., 2022; Ouyang\n",
      "et al., 2022). Whereas these approaches improve or augment the input part of the prompt (e.g.,\n",
      "instructions that are prepended to inputs), our work takes the orthogonal direction of augmenting the\n",
      "outputs of language models with a chain of thought.\n",
      "8 Conclusions\n",
      "We have explored chain-of-thought prompting as a simple and broadly applicable method for enhanc-\n",
      "ing reasoning in language models. Through experiments on arithmetic, symbolic, and commonsense\n",
      "reasoning, we ï¬nd that chain-of-thought reasoning is an emergent property of model scale that allows\n",
      "sufï¬ciently large language models to perform reasoning tasks that otherwise have ï¬‚at scaling curves.\n",
      "Broadening the range of reasoning tasks that language models can perform will hopefully inspire\n",
      "further work on language-based approaches to reasoning.\n",
      "9\n",
      "============================================================\n",
      "[FAISS Retriever]\n",
      "[1] language models can generate chains of thought if demonstrations of chain-of-thought reasoning are\n",
      "provided in the exemplars for few-shot prompting.\n",
      "Figure 1 shows an example of a model producing a chain of thought to solve a math word problem\n",
      "that it would have otherwise gotten incorrect. The chain of thought in this case resembles a solution\n",
      "and can interpreted as one, but we still opt to call it a chain of thought to better capture the idea that it\n",
      "mimics a step-by-step thought process for arriving at the answer (and also, solutions/explanations\n",
      "typically come after the ï¬nal answer (Narang et al., 2020; Wiegreffe et al., 2022; Lampinen et al.,\n",
      "2022, inter alia )).\n",
      "Chain-of-thought prompting has several attractive properties as an approach for facilitating reasoning\n",
      "in language models.\n",
      "1.First, chain of thought, in principle, allows models to decompose multi-step problems into\n",
      "intermediate steps, which means that additional computation can be allocated to problems\n",
      "that require more reasoning steps.\n",
      "2.Second, a chain of thought provides an interpretable window into the behavior of the model,\n",
      "suggesting how it might have arrived at a particular answer and providing opportunities\n",
      "to debug where the reasoning path went wrong (although fully characterizing a modelâ€™s\n",
      "computations that support an answer remains an open question).\n",
      "3.Third, chain-of-thought reasoning can be used for tasks such as math word problems,\n",
      "commonsense reasoning, and symbolic manipulation, and is potentially applicable (at least\n",
      "in principle) to any task that humans can solve via language.\n",
      "4.Finally, chain-of-thought reasoning can be readily elicited in sufï¬ciently large off-the-shelf\n",
      "language models simply by including examples of chain of thought sequences into the\n",
      "exemplars of few-shot prompting.\n",
      "In empirical experiments, we will observe the utility of chain-of-thought prompting for arithmetic\n",
      "reasoning (Section 3), commonsense reasoning (Section 4), and symbolic reasoning (Section 5).\n",
      "3 Arithmetic Reasoning\n",
      "We begin by considering math word problems of the form in Figure 1, which measure the arithmetic\n",
      "reasoning ability of language models. Though simple for humans, arithmetic reasoning is a task where\n",
      "language models often struggle (Hendrycks et al., 2021; Patel et al., 2021, inter alia ). Strikingly, chain-\n",
      "of-thought prompting when used with the 540B parameter language model performs comparably with\n",
      "task-speciï¬c ï¬netuned models on several tasks, even achieving new state of the art on the challenging\n",
      "GSM8K benchmark (Cobbe et al., 2021).\n",
      "3.1 Experimental Setup\n",
      "We explore chain-of-thought prompting for various language models on multiple benchmarks.\n",
      "Benchmarks. We consider the following ï¬ve math word problem benchmarks: (1)theGSM8K\n",
      "benchmark of math word problems (Cobbe et al., 2021), (2)theSV AMP dataset of math word\n",
      "problems with varying structures (Patel et al., 2021), (3)theASDiv dataset of diverse math word\n",
      "problems (Miao et al., 2020), (4)theAQuA dataset of algebraic word problems, and (5)theMA WPS\n",
      "benchmark (Koncel-Kedziorski et al., 2016). Example problems are given in Appendix Table 12.\n",
      "Standard prompting. For the baseline, we consider standard few-shot prompting, popularized by\n",
      "Brown et al. (2020), in which a language model is given in-context exemplars of inputâ€“output pairs\n",
      "before outputting a prediction for a test-time example. Exemplars are formatted as questions and\n",
      "answers. The model gives the answer directly, as shown in Figure 1 (left).\n",
      "Chain-of-thought prompting. Our proposed approach is to augment each exemplar in few-shot\n",
      "prompting with a chain of thought for an associated answer, as illustrated in Figure 1 (right). As most\n",
      "of the datasets only have an evaluation split, we manually composed a set of eight few-shot exemplars\n",
      "with chains of thought for promptingâ€”Figure 1 (right) shows one chain of thought exemplar, and the\n",
      "full set of exemplars is given in Appendix Table 20. (These particular exemplars did not undergo\n",
      "prompt engineering; robustness is studied in Section 3.4 and Appendix A.2.) To investigate whether\n",
      "chain-of-thought prompting in this form can successfully elicit successful reasoning across a range of\n",
      "3\n",
      "[2] C Extended Related Work\n",
      "Chain-of-thought prompting is a general approach that is inspired by several prior directions: prompt-\n",
      "ing, natural language explanations, program synthesis/execution, numeric and logical reasoning, and\n",
      "intermediate language steps.\n",
      "C.1 Prompting\n",
      "The recent success of large-scale language models has led to growing interest in improving their\n",
      "capability to perform tasks via prompting (Brown et al. (2020), and see Liu et al. (2021) for a\n",
      "survey). This paper falls in the category of general prompting approaches, whereby input prompts are\n",
      "optimized to allow a single large language model to better perform a variety of tasks (Li and Liang,\n",
      "2021; Lester et al., 2021; Reif et al., 2022, inter alia ).\n",
      "One recent line of work aims to improve the ability of language models to perform a task by providing\n",
      "instructions that describe the task (Raffel et al., 2020; Wei et al., 2022a; Ouyang et al., 2022; Sanh\n",
      "et al., 2022; Wang et al., 2022b). This line of work is related because it also augments inputâ€“output\n",
      "pairs with meta-data. But whereas an instruction augments the input to a task (instructions are typically\n",
      "prepended to the inputs), chain-of-thought prompting augments the outputs of language models.\n",
      "Another related direction is sequentially combining the outputs of language models; humanâ€“computer\n",
      "interaction (HCI) work (Wu et al., 2022a,b) has shown that combining sequential generations of\n",
      "language models improves task outcomes in a 20-person user study.\n",
      "C.2 Natural language explanations\n",
      "Another closely related direction uses natural language explanations (NLEs), often with the goal of\n",
      "improving model interpretability (Zhou et al., 2020; Wiegreffe and Marasovi Â´c, 2021, inter alia ). That\n",
      "line of work typically focuses on natural language inference (Camburu et al., 2018; Yordanov et al.,\n",
      "2021; Bostrom et al., 2021), and produces explanations either simultaneously to or after the ï¬nal\n",
      "prediction (Narang et al., 2020; Majumder et al., 2021; Wiegreffe et al., 2021, 2022). By contrast,\n",
      "the chain of thought processing considered in this paper occurs before the ï¬nal answer. And while\n",
      "NLE aims mostly to improve neural network interpretability (Rajagopal et al., 2021), the goal of\n",
      "chain-of-thought prompting is to allow models to decompose multi-hop reasoning tasks into multiple\n",
      "stepsâ€”interpretability is just a side effect. Marasovi Â´c et al. (2022) show that prompt-based ï¬netuning\n",
      "with NLE improves NLI and classiï¬cation performance, though they largely focus on evaluating\n",
      "explanation plausibility. In comparison, our work focuses on a range of arithmetic, commonsense,\n",
      "and symbolic tasks that require multi-hop reasoning.\n",
      "C.3 Program synthesis and execution\n",
      "Using intermediate reasoning steps has a long history in program synthesis and execution (Zaremba\n",
      "and Sutskever, 2014, inter alia ). Recent work along in this direction has included a number of\n",
      "architectural innovations (Cai et al., 2017; Dong et al., 2019; Yan et al., 2020), as well as the use of\n",
      "large language models (Chen et al., 2021; Austin et al., 2021). The program execution work closest to\n",
      "ours is perhaps Nye et al. (2021), which show that large language models can perform up to 10-digit\n",
      "addition, evaluate polynomials, and execute python programs. Whereas generating a program and\n",
      "then executing it can be viewed as a type of reasoning, our work generalizes such domain-speciï¬c\n",
      "primitives to natural language, which is open-domain and relevant to any text-to-text NLP task in\n",
      "principle.\n",
      "C.4 Numeric and logical reasoning\n",
      "Numeric and logical reasoning has been a long-studied task in machine learning and natural language\n",
      "processing (Lev et al., 2004, inter alia ). Recent work has also aimed to inject numeric reasoning\n",
      "abilities in language models in various ways, such as augmenting BERT with a predeï¬ned set of\n",
      "executable operations (Andor et al., 2019), including a graph neural network (Ran et al., 2019), and\n",
      "using specialized training procedures (PiË› ekos et al., 2021). Another line of work aims to enable\n",
      "language models to perform logical or formal reasoning, often by verablizing the rules in natural\n",
      "language formal rules using language (Clark et al., 2020; Saeed et al., 2021; Liang et al., 2021).\n",
      "24\n",
      "============================================================\n",
      "[Ensemble Retriever]\n",
      "[1] 0204060GSM8K\n",
      "solve rate (%)LaMDA GPT PaLMStandard prompting\n",
      "Chain-of-thought prompting\n",
      "Prior supervised best\n",
      "020406080SV AMP\n",
      "solve rate (%)\n",
      "0.4 81370255075100MAWPS\n",
      "solve rate (%)\n",
      "0.4 7175 862540\n",
      "Model scale (# parameters in billions)\n",
      "Figure 4: Chain-of-thought prompting enables\n",
      "large language models to solve challenging math\n",
      "problems. Notably, chain-of-thought reasoning\n",
      "is an emergent ability of increasing model scale.\n",
      "Prior best numbers are from Cobbe et al. (2021)\n",
      "for GSM8K, Jie et al. (2022) for SV AMP, and Lan\n",
      "et al. (2021) for MAWPS.Second, chain-of-thought prompting has larger\n",
      "performance gains for more-complicated prob-\n",
      "lems. For instance, for GSM8K (the dataset\n",
      "with the lowest baseline performance), perfor-\n",
      "mance more than doubled for the largest GPT\n",
      "and PaLM models. On the other hand, for Sin-\n",
      "gleOp, the easiest subset of MAWPS which only\n",
      "requires a single step to solve, performance im-\n",
      "provements were either negative or very small\n",
      "(see Appendix Table 3).\n",
      "Third, chain-of-thought prompting via GPT-3\n",
      "175B and PaLM 540B compares favorably to\n",
      "prior state of the art, which typically ï¬netunes a\n",
      "task-speciï¬c model on a labeled training dataset.\n",
      "Figure 4 shows how PaLM 540B uses chain-of-\n",
      "thought prompting to achieve new state of the art\n",
      "on GSM8K, SV AMP, and MAWPS (though note\n",
      "that standard prompting already passed the prior\n",
      "best for SV AMP). On the other two datasets,\n",
      "AQuA and ASDiv, PaLM with chain-of-thought\n",
      "prompting reaches within 2% of the state of the\n",
      "art (Appendix Table 2).\n",
      "To better understand why chain-of-thought\n",
      "prompting works, we manually examined model-\n",
      "generated chains of thought by LaMDA 137B\n",
      "for GSM8K. Of 50 random examples where the\n",
      "model returned the correct ï¬nal answer, all of\n",
      "the generated chains of thought were also log-\n",
      "ically and mathematically correct except two\n",
      "that coincidentally arrived at the correct answer\n",
      "(see Appendix D.1, and Table 8 for examples\n",
      "of correct model-generated chains of thought).\n",
      "We also randomly examined 50 random sam-\n",
      "ples for which the model gave the wrong answer.\n",
      "The summary of this analysis is that 46% of the\n",
      "chains of thought were almost correct, barring\n",
      "minor mistakes (calculator error, symbol map-\n",
      "ping error, or one reasoning step missing), and that the other 54% of the chains of thought had major\n",
      "errors in semantic understanding or coherence (see Appendix D.2). To provide a small insight into\n",
      "why scaling improves chain-of-thought reasoning ability, we performed a similar analysis of errors\n",
      "made by PaLM 62B and whether those errors were ï¬xed by scaling to PaLM 540B. The summary\n",
      "is that scaling PaLM to 540B ï¬xes a large portion of one-step missing and semantic understanding\n",
      "errors in the 62B model (see Appendix A.1).\n",
      "3.3 Ablation Study\n",
      "The observed beneï¬ts of using chain-of-thought prompting raises the natural question of whether the\n",
      "same performance improvements can be conferred via other types of prompting. Figure 5 shows an\n",
      "ablation study with three variations of chain of thought described below.\n",
      "Equation only. One reason for why chain-of-thought prompting might help is that it produces the\n",
      "mathematical equation to be evaluated, and so we test a variation where the model is prompted\n",
      "to output only a mathematical equation before giving the answer. Figure 5 shows that equation\n",
      "only prompting does not help much for GSM8K, which implies that the semantics of the questions\n",
      "in GSM8K are too challenging to directly translate into an equation without the natural language\n",
      "reasoning steps in chain of thought. For datasets of one-step or two-step problems, however, we ï¬nd\n",
      "that equation only prompting does improve performance, since the equation can be easily derived\n",
      "from the question (see Appendix Table 6).\n",
      "5\n",
      "[2] language models can generate chains of thought if demonstrations of chain-of-thought reasoning are\n",
      "provided in the exemplars for few-shot prompting.\n",
      "Figure 1 shows an example of a model producing a chain of thought to solve a math word problem\n",
      "that it would have otherwise gotten incorrect. The chain of thought in this case resembles a solution\n",
      "and can interpreted as one, but we still opt to call it a chain of thought to better capture the idea that it\n",
      "mimics a step-by-step thought process for arriving at the answer (and also, solutions/explanations\n",
      "typically come after the ï¬nal answer (Narang et al., 2020; Wiegreffe et al., 2022; Lampinen et al.,\n",
      "2022, inter alia )).\n",
      "Chain-of-thought prompting has several attractive properties as an approach for facilitating reasoning\n",
      "in language models.\n",
      "1.First, chain of thought, in principle, allows models to decompose multi-step problems into\n",
      "intermediate steps, which means that additional computation can be allocated to problems\n",
      "that require more reasoning steps.\n",
      "2.Second, a chain of thought provides an interpretable window into the behavior of the model,\n",
      "suggesting how it might have arrived at a particular answer and providing opportunities\n",
      "to debug where the reasoning path went wrong (although fully characterizing a modelâ€™s\n",
      "computations that support an answer remains an open question).\n",
      "3.Third, chain-of-thought reasoning can be used for tasks such as math word problems,\n",
      "commonsense reasoning, and symbolic manipulation, and is potentially applicable (at least\n",
      "in principle) to any task that humans can solve via language.\n",
      "4.Finally, chain-of-thought reasoning can be readily elicited in sufï¬ciently large off-the-shelf\n",
      "language models simply by including examples of chain of thought sequences into the\n",
      "exemplars of few-shot prompting.\n",
      "In empirical experiments, we will observe the utility of chain-of-thought prompting for arithmetic\n",
      "reasoning (Section 3), commonsense reasoning (Section 4), and symbolic reasoning (Section 5).\n",
      "3 Arithmetic Reasoning\n",
      "We begin by considering math word problems of the form in Figure 1, which measure the arithmetic\n",
      "reasoning ability of language models. Though simple for humans, arithmetic reasoning is a task where\n",
      "language models often struggle (Hendrycks et al., 2021; Patel et al., 2021, inter alia ). Strikingly, chain-\n",
      "of-thought prompting when used with the 540B parameter language model performs comparably with\n",
      "task-speciï¬c ï¬netuned models on several tasks, even achieving new state of the art on the challenging\n",
      "GSM8K benchmark (Cobbe et al., 2021).\n",
      "3.1 Experimental Setup\n",
      "We explore chain-of-thought prompting for various language models on multiple benchmarks.\n",
      "Benchmarks. We consider the following ï¬ve math word problem benchmarks: (1)theGSM8K\n",
      "benchmark of math word problems (Cobbe et al., 2021), (2)theSV AMP dataset of math word\n",
      "problems with varying structures (Patel et al., 2021), (3)theASDiv dataset of diverse math word\n",
      "problems (Miao et al., 2020), (4)theAQuA dataset of algebraic word problems, and (5)theMA WPS\n",
      "benchmark (Koncel-Kedziorski et al., 2016). Example problems are given in Appendix Table 12.\n",
      "Standard prompting. For the baseline, we consider standard few-shot prompting, popularized by\n",
      "Brown et al. (2020), in which a language model is given in-context exemplars of inputâ€“output pairs\n",
      "before outputting a prediction for a test-time example. Exemplars are formatted as questions and\n",
      "answers. The model gives the answer directly, as shown in Figure 1 (left).\n",
      "Chain-of-thought prompting. Our proposed approach is to augment each exemplar in few-shot\n",
      "prompting with a chain of thought for an associated answer, as illustrated in Figure 1 (right). As most\n",
      "of the datasets only have an evaluation split, we manually composed a set of eight few-shot exemplars\n",
      "with chains of thought for promptingâ€”Figure 1 (right) shows one chain of thought exemplar, and the\n",
      "full set of exemplars is given in Appendix Table 20. (These particular exemplars did not undergo\n",
      "prompt engineering; robustness is studied in Section 3.4 and Appendix A.2.) To investigate whether\n",
      "chain-of-thought prompting in this form can successfully elicit successful reasoning across a range of\n",
      "3\n",
      "[3] experiments on commonsense reasoning underscored how the linguistic nature of chain-of-thought\n",
      "reasoning makes it generally applicable (Section 4). Finally, we showed that for symbolic reasoning,\n",
      "chain-of-thought prompting facilitates OOD generalization to longer sequence lengths (Section 5). In\n",
      "all experiments, chain-of-thought reasoning is elicited simply by prompting an off-the-shelf language\n",
      "model. No language models were ï¬netuned in the process of writing this paper.\n",
      "The emergence of chain-of-thought reasoning as a result of model scale has been a prevailing theme\n",
      "(Wei et al., 2022b). For many reasoning tasks where standard prompting has a ï¬‚at scaling curve, chain-\n",
      "of-thought prompting leads to dramatically increasing scaling curves. Chain-of-thought prompting\n",
      "appears to expand the set of tasks that large language models can perform successfullyâ€”in other\n",
      "words, our work underscores that standard prompting only provides a lower bound on the capabilities\n",
      "of large language models. This observation likely raises more questions than it answersâ€”for instance,\n",
      "how much more can we expect reasoning ability to improve with a further increase in model scale?\n",
      "What other prompting methods might expand the range of tasks that language models can solve?\n",
      "As for limitations, we ï¬rst qualify that although chain of thought emulates the thought processes of\n",
      "human reasoners, this does not answer whether the neural network is actually â€œreasoning,â€ which\n",
      "we leave as an open question. Second, although the cost of manually augmenting exemplars with\n",
      "chains of thought is minimal in the few-shot setting, such annotation costs could be prohibitive for\n",
      "ï¬netuning (though this could potentially be surmounted with synthetic data generation, or zero-shot\n",
      "generalization). Third, there is no guarantee of correct reasoning paths, which can lead to both correct\n",
      "and incorrect answers; improving factual generations of language models is an open direction for\n",
      "future work (Rashkin et al., 2021; Ye and Durrett, 2022; Wiegreffe et al., 2022, inter alia ). Finally,\n",
      "the emergence of chain-of-thought reasoning only at large model scales makes it costly to serve in\n",
      "real-world applications; further research could explore how to induce reasoning in smaller models.\n",
      "7 Related Work\n",
      "This work is inspired by many research areas, which we detail in an extended related work section\n",
      "(Appendix C). Here we describe two directions and associated papers that are perhaps most relevant.\n",
      "The ï¬rst relevant direction is using intermediate steps to solve reasoning problems. Ling et al. (2017)\n",
      "pioneer the idea of using natural language rationales to solve math word problems through a series\n",
      "of intermediate steps. Their work is a remarkable contrast to the literature using formal languages\n",
      "to reason (Roy et al., 2015; Chiang and Chen, 2019; Amini et al., 2019; Chen et al., 2019). Cobbe\n",
      "et al. (2021) extend Ling et al. (2017) by creating a larger dataset and using it to ï¬netune a pretrained\n",
      "language model rather than training a model from scratch. In the domain of program synthesis,\n",
      "Nye et al. (2021) leverage language models to predict the ï¬nal outputs of Python programs via\n",
      "ï¬rst line-to-line predicting the intermediate computational results, and show that their step-by-step\n",
      "prediction method performs better than directly predicting the ï¬nal outputs.\n",
      "Naturally, this paper also relates closely to the large body of recent work on prompting. Since the\n",
      "popularization of few-shot prompting as given by Brown et al. (2020), several general approaches\n",
      "have improved the prompting ability of models, such as automatically learning prompts (Lester et al.,\n",
      "2021) or giving models instructions describing a task (Wei et al., 2022a; Sanh et al., 2022; Ouyang\n",
      "et al., 2022). Whereas these approaches improve or augment the input part of the prompt (e.g.,\n",
      "instructions that are prepended to inputs), our work takes the orthogonal direction of augmenting the\n",
      "outputs of language models with a chain of thought.\n",
      "8 Conclusions\n",
      "We have explored chain-of-thought prompting as a simple and broadly applicable method for enhanc-\n",
      "ing reasoning in language models. Through experiments on arithmetic, symbolic, and commonsense\n",
      "reasoning, we ï¬nd that chain-of-thought reasoning is an emergent property of model scale that allows\n",
      "sufï¬ciently large language models to perform reasoning tasks that otherwise have ï¬‚at scaling curves.\n",
      "Broadening the range of reasoning tasks that language models can perform will hopefully inspire\n",
      "further work on language-based approaches to reasoning.\n",
      "9\n",
      "[4] C Extended Related Work\n",
      "Chain-of-thought prompting is a general approach that is inspired by several prior directions: prompt-\n",
      "ing, natural language explanations, program synthesis/execution, numeric and logical reasoning, and\n",
      "intermediate language steps.\n",
      "C.1 Prompting\n",
      "The recent success of large-scale language models has led to growing interest in improving their\n",
      "capability to perform tasks via prompting (Brown et al. (2020), and see Liu et al. (2021) for a\n",
      "survey). This paper falls in the category of general prompting approaches, whereby input prompts are\n",
      "optimized to allow a single large language model to better perform a variety of tasks (Li and Liang,\n",
      "2021; Lester et al., 2021; Reif et al., 2022, inter alia ).\n",
      "One recent line of work aims to improve the ability of language models to perform a task by providing\n",
      "instructions that describe the task (Raffel et al., 2020; Wei et al., 2022a; Ouyang et al., 2022; Sanh\n",
      "et al., 2022; Wang et al., 2022b). This line of work is related because it also augments inputâ€“output\n",
      "pairs with meta-data. But whereas an instruction augments the input to a task (instructions are typically\n",
      "prepended to the inputs), chain-of-thought prompting augments the outputs of language models.\n",
      "Another related direction is sequentially combining the outputs of language models; humanâ€“computer\n",
      "interaction (HCI) work (Wu et al., 2022a,b) has shown that combining sequential generations of\n",
      "language models improves task outcomes in a 20-person user study.\n",
      "C.2 Natural language explanations\n",
      "Another closely related direction uses natural language explanations (NLEs), often with the goal of\n",
      "improving model interpretability (Zhou et al., 2020; Wiegreffe and Marasovi Â´c, 2021, inter alia ). That\n",
      "line of work typically focuses on natural language inference (Camburu et al., 2018; Yordanov et al.,\n",
      "2021; Bostrom et al., 2021), and produces explanations either simultaneously to or after the ï¬nal\n",
      "prediction (Narang et al., 2020; Majumder et al., 2021; Wiegreffe et al., 2021, 2022). By contrast,\n",
      "the chain of thought processing considered in this paper occurs before the ï¬nal answer. And while\n",
      "NLE aims mostly to improve neural network interpretability (Rajagopal et al., 2021), the goal of\n",
      "chain-of-thought prompting is to allow models to decompose multi-hop reasoning tasks into multiple\n",
      "stepsâ€”interpretability is just a side effect. Marasovi Â´c et al. (2022) show that prompt-based ï¬netuning\n",
      "with NLE improves NLI and classiï¬cation performance, though they largely focus on evaluating\n",
      "explanation plausibility. In comparison, our work focuses on a range of arithmetic, commonsense,\n",
      "and symbolic tasks that require multi-hop reasoning.\n",
      "C.3 Program synthesis and execution\n",
      "Using intermediate reasoning steps has a long history in program synthesis and execution (Zaremba\n",
      "and Sutskever, 2014, inter alia ). Recent work along in this direction has included a number of\n",
      "architectural innovations (Cai et al., 2017; Dong et al., 2019; Yan et al., 2020), as well as the use of\n",
      "large language models (Chen et al., 2021; Austin et al., 2021). The program execution work closest to\n",
      "ours is perhaps Nye et al. (2021), which show that large language models can perform up to 10-digit\n",
      "addition, evaluate polynomials, and execute python programs. Whereas generating a program and\n",
      "then executing it can be viewed as a type of reasoning, our work generalizes such domain-speciï¬c\n",
      "primitives to natural language, which is open-domain and relevant to any text-to-text NLP task in\n",
      "principle.\n",
      "C.4 Numeric and logical reasoning\n",
      "Numeric and logical reasoning has been a long-studied task in machine learning and natural language\n",
      "processing (Lev et al., 2004, inter alia ). Recent work has also aimed to inject numeric reasoning\n",
      "abilities in language models in various ways, such as augmenting BERT with a predeï¬ned set of\n",
      "executable operations (Andor et al., 2019), including a graph neural network (Ran et al., 2019), and\n",
      "using specialized training procedures (PiË› ekos et al., 2021). Another line of work aims to enable\n",
      "language models to perform logical or formal reasoning, often by verablizing the rules in natural\n",
      "language formal rules using language (Clark et al., 2020; Saeed et al., 2021; Liang et al., 2021).\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "sample_query = \"chain-of-thought ê°œë…ì„ ì•Œë ¤ì¤˜\"\n",
    "print(f\"[Query]\\n{sample_query}\\n\")\n",
    "relevant_docs = bm25_retriever.get_relevant_documents(sample_query)\n",
    "print(\"[BM25 Retriever]\")\n",
    "pretty_print(relevant_docs)\n",
    "print(\"===\" * 20)\n",
    "relevant_docs = faiss_retriever.get_relevant_documents(sample_query)\n",
    "print(\"[FAISS Retriever]\")\n",
    "pretty_print(relevant_docs)\n",
    "print(\"===\" * 20)\n",
    "relevant_docs = ensemble_retriever.get_relevant_documents(sample_query)\n",
    "print(\"[Ensemble Retriever]\")\n",
    "pretty_print(relevant_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG í…œí”Œë¦¿ ì‹¤í—˜\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF Path: data2/test_paper_2.pdf\n",
      "ë¬¸ì„œì˜ ìˆ˜: 43\n",
      "============================================================\n",
      "[HUMAN]\n",
      "What is chain-of-thought?\n",
      "\n",
      "[AI]\n",
      "Chain-of-thought is a method that allows models to decompose multi-step problems into intermediate steps, aiding in reasoning tasks. It can be used for tasks such as math word problems, commonsense reasoning, and symbolic manipulation. Language models can generate chains of thought if demonstrations of chain-of-thought reasoning are provided in the exemplars for few-shot prompting.\n"
     ]
    }
   ],
   "source": [
    "# ë‹¨ê³„ 1: ë¬¸ì„œ ë¡œë“œ(Load Documents)\n",
    "# ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³ , ì²­í¬ë¡œ ë‚˜ëˆ„ê³ , ì¸ë±ì‹±í•©ë‹ˆë‹¤.\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# PDF íŒŒì¼ ë¡œë“œ. íŒŒì¼ì˜ ê²½ë¡œ ì…ë ¥\n",
    "file_path = \"data2/test_paper_2.pdf\"\n",
    "loader = PyPDFLoader(file_path=file_path)\n",
    "\n",
    "# ë‹¨ê³„ 2: ë¬¸ì„œ ë¶„í• (Split Documents)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "\n",
    "split_docs = loader.load_and_split(text_splitter=text_splitter)\n",
    "\n",
    "# ë‹¨ê³„ 3, 4: ì„ë² ë”© & ë²¡í„°ìŠ¤í† ì–´ ìƒì„±(Create Vectorstore)\n",
    "# ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# ë‹¨ê³„ 5: ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±(Create Retriever)\n",
    "# ì‚¬ìš©ìì˜ ì§ˆë¬¸(query) ì— ë¶€í•©í•˜ëŠ” ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "# ìœ ì‚¬ë„ ë†’ì€ K ê°œì˜ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.\n",
    "k = 3\n",
    "\n",
    "# (Sparse) bm25 retriever and (Dense) faiss retriever ë¥¼ ì´ˆê¸°í™” í•©ë‹ˆë‹¤.\n",
    "bm25_retriever = BM25Retriever.from_documents(split_docs)\n",
    "bm25_retriever.k = k\n",
    "\n",
    "faiss_vectorstore = FAISS.from_documents(split_docs, OpenAIEmbeddings())\n",
    "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "# initialize the ensemble retriever\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5]\n",
    ")\n",
    "\n",
    "# ë‹¨ê³„ 6: í”„ë¡¬í”„íŠ¸ ìƒì„±(Create Prompt)\n",
    "# í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# ë‹¨ê³„ 7: ì–¸ì–´ëª¨ë¸ ìƒì„±(Create LLM)\n",
    "# ëª¨ë¸(LLM) ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    # ê²€ìƒ‰í•œ ë¬¸ì„œ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ ë¬¸ë‹¨ìœ¼ë¡œ í•©ì³ì¤ë‹ˆë‹¤.\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# ë‹¨ê³„ 8: ì²´ì¸ ìƒì„±(Create Chain)\n",
    "rag_chain = (\n",
    "    {\"context\": ensemble_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "question = \"What is chain-of-thought?\"\n",
    "response = rag_chain.invoke(question)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(f\"PDF Path: {file_path}\")\n",
    "print(f\"ë¬¸ì„œì˜ ìˆ˜: {len(docs)}\")\n",
    "print(\"===\" * 20)\n",
    "print(f\"[HUMAN]\\n{question}\\n\")\n",
    "print(f\"[AI]\\n{response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
